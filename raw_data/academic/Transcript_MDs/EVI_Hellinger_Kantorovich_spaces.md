# Evolutionary Variational Inequalities on the Hellinger-Kantorovich and Spherical Hellinger-Kantorovich spaces

Vaios Laschos<sup>∗</sup> Alexander Mielke†

# 22 December 2023

| 1 | Introduction                                                                                                                                                                                                                                                                                                                                                                       | 2                                      |
|---|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------|
| 2 | Evolutionary variational inequalities versus PDEs                                                                                                                                                                                                                                                                                                                                  | 5                                      |
| 3 | The metric spaces (M(X), HK)<br>and (P(X), SHK)<br>3.1<br>Notation and preliminaries<br>.<br>3.2<br>The logarithmic-entropy transport formulation<br>.<br>3.3<br>Transport-growth systems<br>The Spherical HK<br>distance SHK<br>3.4<br>.                                                                                                                                          | 7<br>7<br>8<br>9<br>11                 |
| 4 | Density bounds for the MM scheme<br>4.1<br>Motivation of density bounds<br>4.2<br>A density estimate for one-step minimizers<br>A single minimization step for HK<br>4.3<br>.<br>A single minimization step for SHK<br>4.4<br>.<br>4.5<br>Many minimization steps<br>.                                                                                                             | 11<br>12<br>13<br>17<br>21<br>25       |
| 5 | Existence for EVI using local κ-concavity<br>5.1<br>Geodesic spaces and the local angle condition (LAC)<br>5.2<br>Semiconvex and semiconcave functions<br>5.3<br>EVIλ<br>and construction of solutions<br>5.4<br>Examples and counterexamples for LAC and semiconcavity<br>.<br>5.5<br>Estimates for the MM scheme<br>5.6<br>Proof of the main abstract result in Theorem 5.9<br>. | 27<br>27<br>29<br>30<br>31<br>34<br>36 |
| 6 | Semiconcavity and EVI flows for (M(X), E,<br>HK)<br>and (P(X), E,<br>SHK)<br>2HK2<br>SHK2<br>Semiconcavity of 1<br>and 1<br>6.1<br>.<br>2<br>Geodesic semiconvexity of functionals on HK<br>and SHK<br>6.2<br>.<br>6.3<br>The Main Result<br>.                                                                                                                                     | 37<br>37<br>39<br>40                   |
| A | Transfer of λ-convexity between HK<br>and SHK                                                                                                                                                                                                                                                                                                                                      | 41                                     |

<sup>∗</sup>WIAS Berlin. Partially supported by DFG under Germany's Excellence Strategy – The Berlin Mathematics Research Center MATH+ (EXC-2046/1, project ID: 390685689).

<sup>†</sup>WIAS Berlin and Institut für Mathematik, Humboldt Unversität zu Berlin. Partially supported by DFG under Priority Program Variational Methods for Predicting Complex Phenomena in Engineering Structures and Materials (SPP-2256, project ID: 441470105, subproject Mi 459/9-1)

#### Abstract

We study the minimizing movement scheme for families of geodesically semiconvex functionals defined on either the Hellinger-Kantorovich space or on the Spherical Hellinger-Kantorovich space. By exploiting some of the finer geometric properties of the spaces (namely the local-angle condition and the semiconcavity of the squared distance, when restricted to suitable subsets), we prove that the sequence of curves, which are produced by geodesically interpolating the points generated by the minimizing movement scheme, converges to a curve that satisfy the Evolutionary Variational Inequality (EVI), when the time step goes to 0. Under suitable conditions, we obtain a global EVI flow on the whole space.

Keywords: Minimizing movement scheme, Evolutionary Variational Inequality, Hellinger-Kantorovich space, Spherical Hellinger-Kantorovich space, density estimates, local-angle condition, semiconcavity of the squared distance.

# 1 Introduction

Let X be a geodesic metric space and M(X) the space of all nonnegative and finite Borel measures on X. Independently, in [CP<sup>∗</sup>15], [KMV16b], and [LMS16, LMS18], the space (M(X),HK) was introduced and studied, where HK denotes the Hellinger-Kantorovich or Wasserstein-Fisher-Rao distance. In [LMS16, LMS18], it was proved that (M(X),HK) is a geodesic space itself and all geodesic curves were characterized. In [LaM19], the spherical Hellinger Kantorovich distance SHK was introduced and it was proved that the set of all probability measures P(X) = µ ∈ M(X) <sup>µ</sup>(X) = 1 endowed with SHK is also a geodesic metric space.

For the rest of the paper, we assume that <sup>X</sup> <sup>⊂</sup> **<sup>R</sup>** d is a compact, convex set with nonempty interior, which allows us to exploit the characterization of geodesic semiconvexity for HK provided recently in [LMS23]. We introduce a family of entropy functionals, i.e.

$$
\mathsf{E}(\mu) = \int_X E(\rho(x)) \mathcal{L}^d(\mathrm{d}x) + E'_{\infty} \mathrm{d}\mu^s, \quad \mu = \rho \mathcal{L}^d + \mu^s \text{ and } \mu^s \perp \mathcal{L}^d,
$$
 (1.1)

where E′ <sup>∞</sup> = limt→∞ E(t) <sup>t</sup> = limt→∞ E′ (t), and µ s the singular part of µ with respect to L d , i.e. the Lebesgue measure restricted at X. In this paper we are going to study De Giorgi's *minimizing movement (MM) scheme*, also known as JKO scheme (after [JKO98]) in the case of the Wasserstein space,

$$
\mu_1 = \inf_{\mu \in \mathcal{M}(X)} \left\{ \frac{\mathsf{H} \mathsf{K}^2(\mu_0, \mu)}{2\tau} + \mathsf{E}(\mu) \right\} \qquad \mu_1 = \inf_{\mu \in \mathcal{P}(X)} \left\{ \frac{\mathsf{H} \mathsf{K}^2(\mu_0, \mu)}{2\tau} + \mathsf{E}(\mu) \right\}, \tag{1.2}
$$

for the Hellinger-Kantorovich and Spherical Hellinger-Kantorovich space respectively. We are going to limit our exploration to cases where the functionals E are of the form (1.1), and satisfy the following basic convexity assumptions.

#### Assumption A

- *1.* E : **R** <sup>+</sup> <sup>→</sup> **<sup>R</sup>** *is a convex function.*
- *2.* <sup>E</sup> *is geodesically* <sup>λ</sup>*-convex for some* <sup>λ</sup> <sup>∈</sup> **<sup>R</sup>***.*

We note that by [AFP00, Theorem 5.2], the functional E is lower semicontinuous, and it is the relaxation of itself when defined only on (Mac(X),HK) or on (Pac(X), SHK).

A metric space along with a lower semicontinuous functional define a metric gradient system, cf. e.g. [Mie23]:

Definition 1.1 *Let* (X, <sup>d</sup>X) *be a metric space and* <sup>φ</sup> : <sup>X</sup> <sup>→</sup> (−∞, <sup>∞</sup>] *a lsc functional, then* (X, φ, dX) *is called a* metric gradient system*.*

The main goal is to show that geodesic interpolation of points that are generated by the MM scheme give rise to sequences of curves with good limiting properties. More specifically, we show that such sequences of curves converge, when τ converges to 0, to curves that satisfy the Evolutionary Variational Inequalities (EVI) for the metric gradient system (M(X), E,HK) or (P(X), E, SHK), respectively.

Before we proceed, we briefly remind the reader of the definition of EVI. It involves the upper right Dini derivative

$$
\frac{\mathrm{d}^+}{\mathrm{d}t}\zeta(t) := \limsup_{h \to 0^+} \frac{1}{h}\left(\zeta(t+h) - \zeta(t)\right).
$$

Definition 1.2 (EVI solutions for metric gradient systems) *Let* (X, φ, dX) *be a* metric gradient system*. For* <sup>T</sup> <sup>∈</sup> (0, <sup>∞</sup>) *and* <sup>λ</sup> <sup>∈</sup> **<sup>R</sup>** *we say that a continuous curve* <sup>x</sup> : [0, T) <sup>→</sup> <sup>X</sup> *is an* EVI<sup>λ</sup> solution *for the metric gradient system* (X, φ, <sup>d</sup>X)*, if* <sup>φ</sup>(x(t)) <sup>&</sup>lt; <sup>∞</sup> *for all* t ∈ (0, T) *and for every "observer"* xob ∈ X, *we have*

$$
\frac{\mathrm{d}^+}{\mathrm{d}t} \frac{1}{2} \mathsf{d}^2_{\mathfrak{X}}(\boldsymbol{x}(t), x_{\text{ob}}) + \frac{\lambda}{2} \mathsf{d}^2_{\mathfrak{X}}(\boldsymbol{x}(t), x_{\text{ob}}) \le \phi(x_{\text{ob}}) - \phi(\boldsymbol{x}(t)) \quad \text{for all } t \in (0, T). \tag{1.3}
$$

*If furthermore* T = ∞, *we call* x *a* complete EVI<sup>λ</sup> solution.

EVIs are used to provide a generalization of the definition of gradient flows in the more abstract setting of geodesic metric spaces, see [AGS05]. For a nice exposition on EVIs the reader is advised to follow the trilogy of papers [MuS20, MuS23]. Our main result (cf. Theorem 6.6) relies on some of these results and reads as follows:

Main Result *Let* <sup>X</sup> <sup>⊂</sup> **<sup>R</sup>** d *be a compact, convex set with nonempty interior. Furthermore, let* E *be of the form* (1.1) *and let it satisfy Assumption A.*

*Then, for all* µ<sup>0</sup> = ρ0L <sup>d</sup> *with* <sup>0</sup> < ρ<sup>0</sup> ≤ ρ0(x) ≤ ρ<sup>0</sup> < ∞ *a.e. in* X*, the geodesically interpolated solutions of the MM schemes corresponding to the gradient system* (M(X), E,HK) *(or* (P(X), E, SHK)*), as in* (1.2)HK *(or* (1.2)SHK*), converge to a complete solution* <sup>µ</sup> : [0, <sup>∞</sup>) <sup>→</sup> <sup>P</sup>(X) *of EVI*λ*. Moreover, for all* <sup>µ</sup><sup>0</sup> <sup>∈</sup> dom(E) HK ⊂ M(X) *(or* <sup>µ</sup><sup>0</sup> <sup>∈</sup> dom(E) SHK ⊂ P(X)*) there exists a unique EVI solution, emanating from* µ0*.*

We emphasize that the generation of EVI solutions presented in [AGS05, Thm. 4.0.4] is not applicable in our case, because it strongly relies on the uniform generalized convexity assumption (cf. [AGS05, Ass. 4.0.1]) for the functional <sup>v</sup> 7→ <sup>1</sup> 2τ d(w, v) <sup>2</sup> + E(v) for d = HK or SHK, respectively. So far, the existence of suitable "generalizations of geodesic curves" as used in the Kantorovich-Wasserstein setting is not known, and there are no ideas how to approach this question in the case of unbalanced transport.

Thus, we follows the completely different route that is devised in [Sav07, Sav11, MuS20, MuS23]. This methods relies on the two independent assumptions of (i) semiconvexity of the functional and (ii) additional properties of the geodesic space (X, d), namely the Local-Angle Condition (LAC) and the semiconcavity of the squared distance, i.e. <sup>1</sup> 2 d 2 (x∗, ·). As these two fundamental properties are not so well-known in the theory of gradient systems, we discuss a few examples in Section 5.4. The semiconcavity for HK and SHK is only true when working on subsets with suitably upper and lower density bounds, see [LaM19]. Our work adapts the results in [MuS23] by suitably accounting for the interplay of localization and semiconcavity with varying parameter κ, where we can use the absolutely non-trivial result from [MuS23] (see [Sav07, Thm. 7] for the first announcement) that the resulting EVI<sup>λ</sup> is independent of the semiconcavity parameter κ.

A family of functionals E satisfying Assumption A on the Hellinger-Kantorovich space (M(X), E) is the following.

Example 1.3 (The case (M(X), E,HK)) *Consider* E γ α,m *generated by* E<sup>γ</sup> α,m(c) = αc<sup>m</sup> + γc *with* α > <sup>0</sup>, m > <sup>1</sup>*, and* <sup>γ</sup> <sup>∈</sup> **<sup>R</sup>***. Then, according to [LMS23, Sec. 7] we know that* <sup>E</sup> *is geodesically* λ*-convex on* (M(X),HK) *with* λ = 2γ*.*

Another example of functionals satisfying Assumption A, on the Hellinger-Kantorovich space but also on the Spherical Hellinger Kantorovich space follows.

Example 1.4 (Both cases (M(X), <sup>E</sup>,HK) and (P(X), <sup>E</sup>, SHK)) *Let* <sup>E</sup>(c) = <sup>−</sup><sup>c</sup> <sup>q</sup> *and assume that either* d = 1 *and* q ∈ [1/3, 1/2] *or* d = 2 *and* q = 1/2*, then [LMS23, Sec. 7] ensures that* E *is geodesically* 0*-convex on* (M(X),HK)*, and our Proposition 6.5 gives the same for* (P(X), SHK)*.*

Open Question 1.5 *In [LMS23] the functionals* E *of the form* (1.1) *that are semiconvex in* (M(X),HK) *were fully characterized. However, for the case of the Spherical Hellinger-Kantorovich space, only very few semiconvexity results are known, and these results are corollaries of general theorems that connect metric spaces with their spherical counterparts, see Proposition 6.5. A general characterization of geodesically semiconvex functionals on the Spherical Hellinger-Kantorovich space is still elusive. We leave this as an open question, and we welcome any suggestion for collaboration in this direction.*

Unlike with other definitions of gradient flows, the EVI approach guarantees some useful properties. One of the most important is the asymptotic stability for sequences of curves that satisfy EVI (see [DaS14, Sec. 2.6] or [Mie23, Thm. 5.12]). More specifically, under weak convergence assumptions for a sequence of functionals Gk, to some G∞, we get for free that the sequence of solutions to the respective EVI converge to a solution of EVI with respect to the limit functional. One can easily show that the limit case EVI for G<sup>m</sup> = E γ α,m for <sup>m</sup> → ∞, where <sup>E</sup> γ α,m as in Example 1.3, corresponds to the functional

$$
\mathsf{E}_{\infty}^{\gamma}(\mu) = \begin{cases} \gamma \mu(X) & \text{for } \mu = \rho \mathcal{L} \quad \text{with } \quad \rho(x) \le 1 \text{ a.e.,} \\ \infty & \text{otherwise,} \end{cases} \tag{1.4}
$$

which was studied independently in [GLM19] and [DiC20].

One of the primary motivations for studying the gradient of entropy functionals on spaces of measures is their frequent identification as solutions to well-known partial differential equations (PDEs). This connection was initially demonstrated for the Fokker-Planck and diffusion equations, which were proven to be gradient flows of the differential entropy on the Wasserstein space. For further details and results, please refer to [JKO98, Ott01, LMS16]. Further applications in [PQV14, DiC20] involve reactiondiffusion equations where the reaction term exactly correspond to the Hellinger part in HK.

Our paper is divided into six sections. In Section 2 we discuss the differences between our EVI approach and other works that provide weak solutions to the corresponding partial differential equations. Gradient flows for HK were already studied in [KMV16a, KMV16b, GaM17, GLM19, DiC20, Fle21] and in [KV19] for SHK. Our approach is more restrictive as it is based on geodesic semiconvexity, but yields uniqueness of solutions and Lipschitz continuous dependence on the initial data, while uniqueness may fail for weak PDE solutions.

In Section 3, we present various equivalent definitions of HK and discuss relevant lemmas. In Section 4, we prove that if µ<sup>0</sup> possesses desirable density bounds, then the minimizers µ<sup>1</sup> in both JKO schemes (1.2) also exhibit favorable density bounds. Notably, for the SHK space, we establish a discrete maximal principle.

Section 5 first introduces the general theory around the Local-Angle Condition (LAC) and the semiconcavity of the squared distance together with some examples in Section 5.4. Then, an abstract existence theorem is developed for EVI (Evolution Variational Inequality) solutions, the proof of which is based on LAC and κ-concavity. Here, we use a series of results from [MuS23] which are cited in full detail but without proof, except for our abstract existence result in Theorem 5.9 that relies on the density of ∪Aκ, where A<sup>κ</sup> are suitable subsets of X in which the squared distance is κ-concave. Our abstract existence result extends the approach provided in [MuS20, MuS23] of proving EVI solutions for λ-convex functionals to situations where κ-concavity of the squared distance is not true globally. We only need that κ-concavity holds only in suitable subsets A<sup>κ</sup> instead of the whole space. We reckon that this localization approach is an interesting extension on its own.

In Section 6 we then show that the necessary κ-concavity for (M(X),HK) and (P(X), SHK) can be obtained from the theory developed in [LaM19]. Combining this with the theory of geodesic convexity of [LMS23] then allows us to establish our main existence results for EVI solutions in the spaces (M(X),HK) and (P(X), SHK).

# 2 Evolutionary variational inequalities versus PDEs

Evolutionary variational inequalities are defined for metric gradient systems (X, E, d), where (X, <sup>d</sup>) is a complete geodesic space and <sup>E</sup> : <sup>X</sup> <sup>→</sup> **<sup>R</sup>** ∪ {∞} a geodesically <sup>λ</sup>-convex functional. A curve u : [0, ∞[ → X is called an EVI<sup>λ</sup> if it satisfies

$$
\forall s, t \ge 0 \text{ with } s < t \forall w \in \text{dom}(\mathsf{E}) :
$$
  
$$
\frac{1}{2} \mathsf{d}^2(u(t), w) \le \frac{1}{2} e^{-\lambda(t-s)} \mathsf{d}^2(u(s), w) + M_\lambda(t-s) \big(\mathsf{E}(w) - \mathsf{E}(u(t)),
$$
 (2.1)

where Mλ(r) = R <sup>r</sup> 0 e <sup>−</sup>λs ds. See Proposition 5.8 and [MuS20] for several equivalent formulations.

To have specific example in mind we consider (X, d) = (M(X),HK) and the functional E from (1.1) with

$$
E(c) = -\sqrt{c} + \frac{1}{3}c^{3/2} \quad \text{for } c \ge 0.
$$
 (2.2)

We restrict to space dimensions d = 1 or 2 and observe that E is geodesically convex (cf. Section 6.2 or [LMS23]. Hence, our main existence results in Theorem 6.6 applies and we have a unique EVI solution for all <sup>µ</sup><sup>0</sup> <sup>∈</sup> <sup>M</sup>(X). Here, we use that dom(E) = L<sup>3</sup>/<sup>2</sup> (X) is dense in (M(X),HK).

We claim that the unique solution starting in µ<sup>0</sup> = 0 is given by the curve

$$
\widetilde{\mu}(t) = \rho(t) \mathcal{L}^d(\mathrm{d}x)
$$
 with  $\rho(t) = \tanh^2(t)$ .

It is difficult to check that this µ satisfies (2.1). However, [MuS20, Thms. 3.5, cf. (3.17)] proves that all EVI solutions µ : [0, ∞[ → M(X) are curves of maximal slope , i.e.

$$
\frac{d}{dt}E(\mu(t)) = -\frac{1}{2}|\dot{\mu}|_{\mathbf{H}}^2(t) - \frac{1}{2}|\partial E|_{\mathbf{H}}^2(\mu(t)) \quad \text{for } t > 0.
$$
 (2.3)

More importantly. [MuS20, Thms. 4.2] states the reverse: if the EVI flow exists on dom(E) then every curve of maximal slope is an EVI solution. Thus, <sup>µ</sup><sup>e</sup> the unique EVI solution starting at µ<sup>0</sup> = 0, if it is a curve of maximal slope for M(X), E,HK).

To check this, we first observe that [0, S∗] ∋ s 7→ s <sup>2</sup>µ<sup>∗</sup> is a HK geodesics for all <sup>µ</sup><sup>∗</sup> <sup>∈</sup> <sup>M</sup>(X) one finds HK(<sup>r</sup> 2 0µ∗, r<sup>2</sup> <sup>1</sup>µ∗) = |r0−r1|µ∗(X) 1/2 . Applying this to <sup>µ</sup><sup>e</sup> we find HK(µe(t1), <sup>µ</sup>e(t2)) = tanh(t1) − tanh(t2)  L d (X) <sup>1</sup>/<sup>2</sup> and conclude that the metric speed satisfies

$$
|\dot{\tilde{\mu}}|_{\mathbf{H}}^2(t) = (\tanh'(t))^2 \mathcal{L}^d(X) = (1 - \tanh^2(t))^2 \mathcal{L}^d(X).
$$

Next we observe that <sup>E</sup>(µe(t)) = <sup>L</sup> d (X)( <sup>1</sup> 3 tanh<sup>3</sup> (t) − tanh t) such that

$$
\frac{\mathrm{d}}{\mathrm{d}t} \mathsf{E}(\widetilde{\mu}(t)) = -\tanh'(t) \big(1 - \tanh^2(t)\big) = -\big(1 - \tanh^2(t)\big)^2.
$$

Finally, from <sup>ξ</sup> = DE(µe(t)) and <sup>|</sup>∂E<sup>|</sup> 2 HK = R X ρ|∇ξ| <sup>2</sup> + 4ρξ<sup>2</sup> dx, we obtain, with ξ = E′ (ρ) = <sup>1</sup> 2 (ρ(t)−1)/(2<sup>p</sup> ρ(t)), the metric slope

$$
\big|\partial\mathsf{E}\big|^2_{\mathsf{HK}}(\widetilde{\mu})=4\rho\Big(\frac{\rho-1}{2\sqrt{\rho}}\Big)^2=(\rho(t)-1)^2=\big(1-\tanh^2(t)\big)^2.
$$

Thus, <sup>µ</sup><sup>e</sup> is shown to be a curve of maximal slope, which is unique because of the uniqueness of EVI solutions.

Of course, the EVI formulation can be compared to corresponding weak formulations of the associated partial differential equation occurring to as gradient-flow equations. Generalizing the above energy to a sum of an internal energy E and a potential energy <sup>µ</sup> 7→ − <sup>R</sup> X V µ(dx) we consider the gradient systems (M(X), F,HKα) and P(X), F, SHKα,β) with <sup>F</sup>(µ) = <sup>E</sup>(µ) <sup>−</sup> R X V µ(dx). Here, HKα,β and SHKα,β are scaled versions of HK = HK1,<sup>4</sup> and SHK = SHK1,4, respectively, that are defined via the Onsager operators

$$
\mathbb{K}^{\mathbf{H}}_{\alpha,\beta}(\mu)\,\xi = -\,\mathrm{div}\,(\alpha\rho\nabla\xi) + \beta\rho\xi,
$$
  
$$
\mathbb{K}^{\mathbf{H}}_{\alpha,\beta}(\mu)\,\xi = -\,\mathrm{div}\,(\alpha\rho\nabla\xi) + \beta\rho\left(\xi - \int_X \rho\xi\mathrm{d}x\right).
$$

Assuming further that L d is the d-dimensional Lebesgue measure (restricted to X) such that E(µ) = R <sup>X</sup> <sup>E</sup>(ρ)d<sup>x</sup> we have the derivative <sup>D</sup>E(ρdx) = <sup>E</sup>′ (ρ) and the Otto calculus (see [Ott01, AGS05, LiM13, LMS16]) leads to the gradient-flow equation

$$
\dot{\rho} = -\mathbb{K}_{\alpha,\beta}^{\mathbf{H}}(\mu) \mathrm{D}F(\mu) = \alpha \mathrm{div}(\rho \nabla (E'(\rho) - V)) - \beta \rho (E'(\rho) - V),
$$
  
\n
$$
\dot{\rho} = -\mathbb{K}_{\alpha,\beta}^{\mathbf{H}}(\mu) \mathrm{D}F(\mu) = \alpha \mathrm{div}(\rho \nabla (E'(\rho) - V)) - \beta \rho (E'(\rho) - V - \int_X \rho (E'(\rho) - V) \mathrm{d}x).
$$

The existence of weak solutions to the above reaction-diffusion equations was established for various cases already. The gradient structure in (M(X),HK) was first exploited in [KMV16a, KMV16b], for models in which existence of solutions follows from classical PDE techniques. In [GaM17] the convergence of a time-splitting scheme is shown, were minimizing movements steps are alternating between the Kantorovich-Wasserstein distance and the Hellinger-Fisher-Rao distance. In [Fle21] the static characterization of the HK distance from [LMS16, LMS18] is employed for showing that the curves generated by the JKO scheme converge to solutions of the above reaction-diffusion PDE.

The gradient-flow equations for (P(X), F, SHK) were studied in [KV19], where existence is established via approximation by classical solutions and suitable a priori estimates. Moreover, convergence into steady states as well as functional inequalities are obtained.

The special gradient system (M(X), <sup>E</sup>,HK) with <sup>E</sup> from (2.2), <sup>V</sup> <sup>≡</sup> <sup>0</sup>, and (α, β) = (1, 4) leads to the above EVI from the beginning of this section. Recall that this EVI has a unique solution starting at µ = 0. The associated PDE for this case reads

$$
\dot{\rho} = \Delta \left(\frac{1}{2}\sqrt{\rho} + \frac{1}{6}\rho^{3/2}\right) + 2\left(\sqrt{\rho} - \rho^{3/2}\right) \text{ in } X, \quad \nabla \rho \cdot \nu = 0 \text{ on } \partial X,\tag{2.4}
$$

which is indeed a parabolic equation (with fast diffusion). However, this equation is non-Lipschitz at ρ = 0 and admits a one-parameter family of spatially constant solutions, namely for ζ ≥ 0

$$
\rho^{(\zeta)}(t) = \begin{cases} \tanh^2(t-\zeta) & \text{for } t \ge \zeta, \\ 0 & \text{for } t \in [0,\zeta]. \end{cases}
$$

Clearly, the solutions are not curves of maximal slope because they are not EVI solutions (cf. [MuS20, Thms. 3.5+4.2]). This, we see a clear difference between the set of EVI solutions and PDE solutions. It is expected, but still unproved, that the generalized minimizing movements obtained in [Fle21] coincide with the EVI solutions.

# 3 The metric spaces (M(X), HK) and (P(X), SHK)

### 3.1 Notation and preliminaries

We will denote by M(X) the space of all nonnegative and finite Borel measures on X endowed with the weak topology induced by the duality with the continuous and bounded functions of Cb(X). The subset of measures with finite quadratic moment will be denoted by M2(X). The spaces P(X) and P2(X) are the corresponding subsets of probability measures. If µ ∈ M(X) and T : X → X is a Borel map, T♯µ will denote the pushforward measure on M(X), defined by

$$
T_{\sharp}\mu(B) := \mu(T^{-1}(B)) \quad \text{for every Borel set } B \subset X. \tag{3.1}
$$

We will often denote elements of X × X by (x0, x1) and the canonical projections by π i : (x0, x1) → x<sup>i</sup> for i = 0, 1. A transport plan on X is a measure M<sup>01</sup> ∈ M(X×X) with marginals µ<sup>i</sup> := π i <sup>♯</sup>M01.

### 3.2 The logarithmic-entropy transport formulation

Here we first provide the definition of the HK(µ0, µ1) distance in terms of a minimization problem that balances a specific transport problem of measures σ0µ<sup>0</sup> and σ1µ<sup>1</sup> with the relative entropies of σjµ<sup>j</sup> with respect to µ<sup>j</sup> . For the characterization of the Hellinger– Kantorovich distance via the static Logarithmic-Entropy Transport (LET) formulation, we define the logarithmic entropy density F : [0, ∞[ → [0, ∞[ via F(r) = r log r−r+1 and the cost function <sup>ℓ</sup> : [0, <sup>∞</sup>[ <sup>→</sup> [0, <sup>∞</sup>] via <sup>ℓ</sup>(R) = <sup>−</sup>2 log (cos (R)) for R < <sup>π</sup> 2 and ℓ ≡ +∞ otherwise. For given measures <sup>µ</sup>0, µ<sup>1</sup> the LET functional LET(t; <sup>µ</sup>0, µ1) : <sup>M</sup>(<sup>X</sup> <sup>×</sup> <sup>X</sup>) <sup>→</sup> [0, ∞[ reads

$$
\mathsf{LET}(H_{01}; \mu_0, \mu_1) := \int_X F(\sigma_0) \mathrm{d}\mu_0 + \int_X F(\sigma_1) \mathrm{d}\mu_1 + \iint_{X \times X} \ell(\mathsf{d}_X(x, x_1)) \mathrm{d}H_{01} \tag{3.2}
$$

with η<sup>i</sup> := (πi)♯H<sup>01</sup> = σiµ<sup>i</sup> ≪ µ<sup>i</sup> . With this, the equivalent formulation of the Hellinger– Kantorovich distance as entropy-transport problem reads as follows.

Theorem 3.1 (LET formulation, [LMS18, Sec. 5]) *For all* µ0, µ<sup>1</sup> ∈ M(X) *we have*

$$
\mathsf{H}^{2}(\mu_{0}, \mu_{1}) = \min \left\{ \mathsf{L}\mathsf{F}(H_{01}; \mu_{0}, \mu_{1}) \, \middle| \, H_{01} \in \mathcal{M}(X \times X), \, (\pi_{i})_{\sharp} H_{01} \ll \mu_{i} \right\}.
$$
 (3.3)

An optimal transport plan H01, which always exists, gives the effective transport of mass. Note, in particular, that only η<sup>i</sup> ≪ µ<sup>i</sup> is required and the cost of a deviation of ηi from µ<sup>i</sup> is given by the entropy functionals associated with F. Moreover, the cost function ℓ is finite in the case dX(x0, x1) < π 2 , which highlights the sharp threshold between transport and pure absorption-generation mentioned earlier.

Amongst the many characterizations of HK discussed in [LMS18] there is one that connects HK with the classical Kantorovich-Wasserstein distance on the cone C over the base space (X, dX) with metric

$$
\mathsf{d}_{\mathfrak{C}}^2(z_0, z_1) := r_0^2 + r_1^2 - 2r_0r_1\cos_{\pi/2}(\mathsf{d}_X(x_0, x_1)), \quad z_i = [x_i, r_i], \tag{3.4}
$$

where as above cosb(a) = cos(min{b, a}). Measures in M(X) can be "lifted" to measures in M(C), e.g. by considering the measure µ ⊗ δ<sup>1</sup> for µ ∈ M(X). Moreover, we can define the projection of measures in M2(C) onto measures in M(X) via

$$
\mathfrak{P}: \left\{ \begin{array}{ccc} \mathfrak{M}_2(\mathfrak{C}) & \to & \mathfrak{M}(X), \\ \lambda & \mapsto & \int_{r=0}^{\infty} r^2 \lambda(\cdot, \mathrm{d}r). \end{array} \right.
$$

For example, the lift λ = m0δ{0} + µ ⊗ 1 r(·) <sup>2</sup> δr(·) , with m<sup>0</sup> ≥ 0 and r : supp(µ) → ]0, ∞[ arbitrary, gives Pλ = µ. Now, the cone space formulation of the Hellinger–Kantorovich distance of two measures µ0, µ<sup>1</sup> ∈ M(X) is given as follows.

Theorem 3.2 (Optimal transport formulation on the cone) *For* µ0, µ<sup>1</sup> ∈ M(X) *we have*

$$
\mathsf{H}^{2}(\mu_{0}, \mu_{1}) = \min \left\{ \mathsf{W}_{\mathsf{d}_{\mathfrak{C}}}^{2}(\lambda_{0}, \lambda_{1}) \Big| \lambda_{i} \in \mathcal{P}_{2}(\mathfrak{C}), \ \mathfrak{P}\lambda_{i} = \mu_{i} \right\}
$$
  
= 
$$
\min \left\{ \iint_{\mathfrak{C} \times \mathfrak{C}} d_{\mathfrak{C}}^{2}(z_{0}, z_{1}) d \Lambda_{01}(z_{0}, z_{1}) \Big| \pi_{\sharp}^{i} \Lambda_{01} = \lambda_{i}, \ \text{and } \mathfrak{P}\lambda_{i} = \mu_{i} \right\}.
$$

This result will be needed for proving K-semiconcavity in Theorem 6.1.

### 3.3 Transport-growth systems

As a slight generalization of [LMS23, Def. 2.7] we define transport-growth couples which generalizes the transport map for the pure Kantorovich-Wasserstein case and allows for growth (or decay) of mass as well.

Definition 3.3 *[Transport-growth couple] A quintuple* (ν, q0, T0, q1, T1) *with* ν ∈ M(Y)*,* Ti : Y → X*, and* q<sup>i</sup> ∈ L 2 (Y ; ν) *is called transport-growth couple for* (µ0, µ1)*, if*

$$
(\mathbf{T}_i)_{\sharp}(q_i^2 \nu) = \mu_i. \tag{3.5}
$$

*is satisfied. If the transport-growth couple, has the form* (µ0, 1, I, q, T), *then we call* (q, T) *a transport-growth couple from* µ<sup>0</sup> *to* µ1*. If for the transport-growth couple we have*

$$
\mathsf{H}^{2}(\mu_{0}, \mu_{1}) = \int_{Y} \left( (q_{0} - q_{1})^{2} + 4q_{0}q_{1} \sin^{2}(|\mathbf{T}_{0} - \mathbf{T}_{1}|/2 \wedge \pi/2) \right) d\nu, \tag{3.6}
$$

*then we will call that an* optimal transport-growth couple *from* µ<sup>0</sup> *to* µ1*.*

Definition 3.4 *For* µ0, µ<sup>1</sup> ∈ M(X), *we define the following sets:*

$$
A'_{i} = \{ x \in X : \text{dist}(x, \text{supp}(\mu_{1-i} < \pi/2)) \}, \qquad A''_{i} := X \setminus A'_{i}. \tag{3.7}
$$

*We also define the following measure*

$$
\mu_i'(t) := (\mu_i)_{|_{A_i'}}(t) = \mu_i(t \cap A_i'), \qquad \mu_i''(t) := (\mu_i)_{|_{A_i''}}(t) = \mu_i(t \cap A_i). \tag{3.8}
$$

Definition 3.5 (Reduced couple) *A couple of measures* (µ0, µ1) ∈ M(X) 2 *is called reduced if* µ<sup>0</sup> = µ ′ 0 , µ<sup>1</sup> = µ ′ 1 .

For every couple (µ0, µ1), the couple (µ ′ 0 , µ′ 1 ) is always reduced. Now, we have the following theorem that is a simplified version of [LMS23, Corollary 3.5].

Theorem 3.6 *Let* <sup>X</sup> <sup>⊂</sup> **<sup>R</sup>** d *be a compact, convex set with nonempty interior. Let* (µ0, µ1) ∈ M(X) 2 , *and* µ<sup>0</sup> ≃ L d . *Then there exists an optimal transport-growth couple* (q, T) *from* µ<sup>0</sup> *to* µ1, *with* |T(x)−x| < π/2 L d *-a.e. If furthermore* µ ′ <sup>1</sup> ≪ L d , *then* Te *is essentially injective.*

Remark 3.7 *A transport plan* T *as in Theorem 3.6 has a version that is fully injective. From now on, without loss of generality we will make the assumption that* T *is fully injective to simplify the arguments. Even more, a couple like that, will be called an injective optimal transport-growth couple from* µ<sup>0</sup> *to* µ1.

In the next lemma, we will show that if the couple (q, T) is an injective optimal transport-growth couple from µ<sup>0</sup> to µ1, then it acts as an injective optimal transportgrowth couple from (µ0)<sup>|</sup><sup>A</sup> to (µ1)<sup>|</sup><sup>T</sup> (A) , for every measurable set A. Even more for any partition {Ai} of X, the total transport-growth cost squared is equal to the sum of the squares of the transport-growth costs for each part of the partition. This straightforward lemma will be used in the next section, for the construction of measures that violate the minimum assumption for the MM scheme if the minimum candidate does not have nice density bounds. These construction will be achieved by cutting and gluing the potential candidate with other measures.

Lemma 3.8 *Let* (µ0, µ1) ∈ M(X) <sup>2</sup> *with* µ ′ 0 , µ′ <sup>1</sup> ≪ L <sup>d</sup> *and* µ ′′ <sup>1</sup> = 0*. Let* (q, T) *be an injective optimal transport-growth couple from* µ<sup>0</sup> *to* µ<sup>1</sup> *and let* A<sup>i</sup> , i = 1, . . . , n, *be a partition of* X*. If* µ i 0 *is the restriction of* µ<sup>0</sup> *on* A<sup>i</sup> , *and* µ i 1 , *the restriction of* µ<sup>1</sup> *on* T(Ai), *then we have:*

- (q, T) *is an injective optimal transport-growth couple from* µ i 0 *to* µ i 1 *.*
- P i µ i <sup>1</sup> = µ1.
- HK<sup>2</sup> (µ0, µ1) = P<sup>n</sup> <sup>i</sup>=1 HK<sup>2</sup> (µ i 0 , µ<sup>i</sup> 1 ).

Proof. We have

$$
\int_{X} \zeta(x) \mu_{1}^{i}(\mathrm{d}x) = \int_{X} \mathbb{I}_{T(A^{i})}(x) \zeta(x) \mu_{1}(\mathrm{d}x) = \int_{X} \mathbb{I}_{T(A^{i})}(T(x)) \zeta(T(x)) q^{2}(x) \mu_{0}(\mathrm{d}x)
$$
\n
$$
= \int_{(X \setminus A^{i})} \mathbb{I}_{T(A^{i})}(T(x)) \zeta(T(x)) q^{2}(x) \mu_{0}(\mathrm{d}x) + \int_{A_{i}} \mathbb{I}_{T(A^{i})}(T(x)) \zeta(T(x)) q^{2}(x) \mu_{0}(\mathrm{d}x)
$$
\n
$$
= 0 + \int_{A_{i}} \zeta(T(x)) q^{2}(x) \mu_{0}(\mathrm{d}x) = \int_{X} \zeta(T(x)) q^{2}(x) \mu_{0}^{i}(\mathrm{d}x).
$$

By summing over i, we also obtain Pµ i <sup>1</sup> = µ1. Since |T(x)−x| < π 2 , it holds that for every i ∈ 1, . . . , n, the couple (µ i 0 , µ<sup>i</sup> 1 ) is reduced and we can construct an optimal transportgrowth couple (q i , T i ) for (µ i 0 , µ<sup>i</sup> 1 ), with cost, HK<sup>2</sup> (µ i 0 , µ<sup>i</sup> 1 ). For every i, we have that (q, T) is a transport-growth couple between µ i 0 , and µ i 1 . Therefore we have

$$
\begin{split} \mathsf{H}\mathsf{K}^{2}(\mu_{0}^{i}, \mu_{1}^{i}) &= \int_{X} \left(1 + q^{i}(x)^{2} - 2q^{i}(x)\cos(|x - \mathbf{T}^{i}(x)|)\right) \mu_{0}^{i}(\mathrm{d}x) \\ &\leq \int_{X} \left(1 + q^{2}(x) - 2q(x)\cos(|x - \mathbf{T}(x)|)\right) \mu_{0}^{i}(\mathrm{d}x). \end{split} \tag{3.9}
$$

We now define (q, <sup>e</sup> <sup>T</sup>e) by

$$
\widetilde{T}(x) = T^i(x), \quad x \in A^i, \qquad \widetilde{q}(x) = q^i(x), \quad x \in A^i,
$$
\n(3.10)

we have

$$
\mu_1 = \sum_{i=1}^n \mu_1^i = \sum_{i=1}^n \widetilde{T}_{\#}(\widetilde{q}^2 \mu_0^i) = \widetilde{T}_{\#}(\widetilde{q}^2 \mu_0),
$$

and therefore it is a transport-growth couple for (µ0, µ1) with total cost

$$
\sum_{i=1}^{n} \mathsf{H}^{2}(\mu_{0}^{i}, \mu_{1}^{i}) \leq \sum_{i=1}^{n} \int_{X} \left(1 + q^{2}(x) - 2q(x)\cos\left(|x - \mathbf{T}(x)|\right)\right) \mu_{0}^{i}(\mathrm{d}x) = \mathsf{H}^{2}(\mu_{0}, \mu_{1})
$$
\n(3.11)

If at least one of the estimates in 3.9 is strict then the above inequality is also strict which implies that (q, <sup>e</sup> <sup>T</sup>e) is a transport-growth couple that has less cost than (q, <sup>T</sup>), which contradicts the fact that the latter is optimal. From the above we have that (3.9) is an equality for every i, and therefore (q, T) is an injective optimal transport-growth couple between µ i 0 , and µ i 1 , and P<sup>n</sup> <sup>i</sup>=1 HK<sup>2</sup> (µ i 0 , µ<sup>i</sup> 1 ) = HK<sup>2</sup> (µ0, µ1).

We will also use the following results, which was discussed carefully in [LaM19].

Proposition 3.9 (Scaling property of HK) *For all* <sup>µ</sup>0, µ<sup>1</sup> <sup>∈</sup> <sup>M</sup>(X) *and* <sup>t</sup>0, t<sup>1</sup> <sup>≥</sup> <sup>0</sup> *we have*

$$
\mathsf{H}^{2}(t_{0}^{2}\mu_{0}, t_{1}^{2}\mu_{1}) = t_{0}t_{1}\mathsf{H}^{2}(\mu_{0}, \mu_{1}) + (t_{0}^{2}-t_{0}t_{1})\mu_{0}(X) + (t_{1}^{2}-t_{0}t_{1})\mu_{1}(X). \tag{3.12}
$$

*Even more, if* H<sup>01</sup> *is an optimal plan for the* LET *formulation of* HK(µ0, µ1), *then* H t0t<sup>1</sup> <sup>01</sup> = t0t1H<sup>01</sup> *is an optimal plan for* HK(t 2 0µ0, t<sup>2</sup> <sup>1</sup>µ1).

Choosing t0/t<sup>1</sup> = (µ1(X)/µ0(X))<sup>1</sup>/<sup>2</sup> in (3.12) we obtain the lower bound

$$
\mathsf{H}\mathsf{K}^2(\mu_0, \mu_1) \ge \left(\sqrt{\mu_0(X)} - \sqrt{\mu_1(X)}\right)^2. \tag{3.13}
$$

# 3.4 The Spherical HK distance SHK

The Spherical Hellinger-Kantorovich space (P(X), SHK) was introduced in [LaM19], and the distance metric is related to the Hellinger-Kantorovich distance HK restricted to P(X)×P(X) through the formula by

$$
\mathbf{SK}(\nu_0,\nu_1)=\arccos\left(1-\frac{1}{2}\,\mathsf{H}\mathsf{K}^2(\nu_0,\nu_1)\right)=2\arcsin\left(\frac{1}{2}\,\mathsf{H}\mathsf{K}(\nu_0,\nu_1)\right).
$$

The important point is that (P(X), SHK) is still a geodesic space in the sense of Definition 5.1. Furthermore, the work [LaM19] demonstrates how all geodesics connecting ν<sup>0</sup> and ν<sup>1</sup> in (P(X), SHK) can be obtained by suitably projecting, which involves dividing by the total mass, and reparametrizing the geodesics in (M(X),HK).

# 4 Density bounds for the MM scheme

The objective of this section is to demonstrate that when starting with a µ<sup>0</sup> exhibiting favorable density bounds, the minimizers of the MM schemes (1.2) also possess similar bounds. These bounds will later be utilized to recover concavity properties for the squared distance along the geodesics that interpolate the points generated by the scheme. The approach is an extension of an idea by Felix Otto developed in [Ott96]. There, the author proved that for a specific class of functionals E, and for measures µ<sup>0</sup> that have density bounded from below by a number cmin > 0, the one step minimizer of the MM scheme, <sup>µ</sup><sup>1</sup> = arg min <sup>n</sup> W2(µ0,µ) <sup>2</sup><sup>τ</sup> <sup>+</sup> <sup>E</sup>(µ) o , has also the same property. The main argument was, that if the set of points with density smaller than cmin is not essential empty, then mass must have moved outside from it to an another set, resulting in density bigger than cmin. This would imply, that keeping some of the mass at place would not only have been cheaper with respect to the Wasserstein distance, but also would have resulted to a more "uniform" distribution of the density and therefor a smaller value of the functional, getting a contradiction to the assumption that µ<sup>1</sup> is a minimizer. Felix Otto, used this argument in [Ott96] to prove that the MM scheme for the p-density functionals, converge to solutions of the doubly degenerate diffusion equations. Our arguments are of similar nature, although we have to take into account that our setting also allows for destruction and creation of mass.

### 4.1 Motivation of density bounds

When analyzing the minimizing movement scheme for diffusion equations in the Kantorovich-Wasserstein setting, comparison principles for parabolic equations play a crucial role. When adding reaction terms like for HK or SHK gradient-flow equations, it is important to see how these comparison principles can still be exploited. This is especially nontrivial in the case of SHK, where a nonlocal term appears.

As the diffusion is well understood, it is worthwhile to look at the pure ODE case. Consider any convex domain <sup>X</sup> <sup>⊂</sup> **<sup>R</sup>** <sup>d</sup> with L d (X) = 1. Moreover, we restrict our view to measures with spatially constant Lebesgue density, i.e. µ(t) = c(t)L <sup>d</sup> as special solutions for the gradient system (M(X), E,HK). Clearly, the equation for the scalar c is

$$
\dot{c} = -4 \, c \, E'(c).
$$

Because of the above choices we have HK(c0L d , c1L d ) = √ c1− √ c0 2 , and the MM scheme reduces to

$$
\frac{1}{2\tau} \left( \sqrt{c_1} - \sqrt{c_0} \right)^2 + E(c_1) \sim \min_{c_1 \ge 0} \quad \longleftrightarrow \quad 1 - \sqrt{\frac{c_0}{c_1}} + 2\tau E'(c_1) = 0. \tag{4.1}
$$

Assuming E′ (clow) ≤ 0 ≤ E′ (cupp) we obtain the following trivial observations for the MM scheme solutions:

(D1) 
$$
c_0 \ge c_{\text{upp}} \iff c_1 \le c_0,
$$
  
(D2)  $c_0 \le c_{\text{low}} \iff c_1 \ge c_0.$ 

However, in the case inbetween, we obtain nontrivial estimates:

(D3) 
$$
c_1 \le \max\left\{a, \frac{c_0}{(1+2\tau \min\{E'(a), 0\})^2}\right\}
$$
 whenever  $2\tau E'(a) > -1$ ;  
(D4)  $c_1 \ge \min\left\{b, \frac{c_0}{(1+2\tau \max\{E'(b), 0\})^2}\right\}$  for all  $b \ge 0$ .

To see that the upper estimate in (D3) holds we set µ<sup>a</sup> := min{E′ (a), 0} with 0 ≥ µ<sup>a</sup> > −1/(2τ ) and assume that (D3) does not hold, i.e. (i) c<sup>1</sup> > a and (ii) c<sup>1</sup> > c0/(1+2τµa) −2 . Then, by (i) and monotonicity of E′ we have 0 ≤ (1+2τµa) <sup>2</sup> ≤ (1+2τE′ (c1))<sup>2</sup> . Exploiting the Euler-Lagrange equation in (4.1) we continue

$$
(1+2\tau\mu_a)^2 \le (1+2\tau E'(c_1))^2 \stackrel{\text{EL eqn.}}{=} c_0/c_1 \stackrel{\text{(ii)}}{<} (1+2\tau\mu_a)^2,
$$

which is the desired contradiction. The lower estimate in (D4) follows similarly.

We have considered the simple ODE case because it turns out that similar estimates hold for the densities in a true minimization step for (M(X), E,HK), see (4.9) in Proposition 4.2 for the upper estimate (D3) and Proposition 4.3 for the lower estimate (D4).

The "spherical" case for (M(X), E, SHK) is in fact much better, because no sign conditions for E′ (c) are needed. To see this we again consider the pure Spherical Hellinger space (P(X), E, SHe) with SHe(ν0, ν1) = 2 arcsin He(ν0, ν1)/2 and He(µ0, µ1) <sup>2</sup> = R X <sup>d</sup>µ<sup>0</sup> dµ <sup>1</sup>/<sup>2</sup> <sup>−</sup> dµ<sup>0</sup> dµ <sup>1</sup>/<sup>2</sup> 2 dµ for any µ with µ0+µ<sup>1</sup> ≪ µ. The corresponding gradient flow for for absolutely continuous measures ν(t) = c(, ·)dx leads to

$$
\dot{c}(t,x) = -4c(t,x)\Big(E'(c(t,x)) - \int_X c(t,y)E'(c(t,y)) dy\Big).
$$

For this flow it can be shown that t 7→ inf c(t, ·) is increasing and t 7→ sup c(t, ·) is decreasing. For this we consider any smooth convex function <sup>ϕ</sup> : ]0, <sup>∞</sup>[ <sup>→</sup> **<sup>R</sup>** and observe

$$
\frac{1}{4} \frac{d}{dt} \int_X \varphi(c(t, x)) dx = \frac{1}{4} \int_X \varphi'(c) \dot{c} dx = \int_X E'(c) c dx \int_X \varphi'(c) c dx - \int_X E'(c) \varphi'(c) c dx
$$
$$
= \int_0^\infty E'(c) dF_t(c) \int_0^\infty \varphi'(c) dF_t(c) - \int_0^\infty E'(c) \varphi'(c) dF_t(c) \le 0,
$$

where Ft(b) := R X c(t, x)11c(t,·)≤b(x) dx with 0 ≤ Ft(c) ≤ Ft(∞) = 1. The estimate ∗ ≤ is a well-known rearrangement estimate following from the monotonicities of E′ (c) and ϕ ′ , see [HLP34, Ch. 10.13].

Given c<sup>0</sup> with c ≤ c0(x) ≤ c we set c<sup>∗</sup> = 1 2 c+c and δ = c − c∗. For p ≥ 2 we choose ϕ(c) = |c−c∗| p . Using L d (X) = 1 again we find kc(t)−c∗kL<sup>p</sup> ≤ kc0−c∗kL<sup>p</sup> ≤ δ. In the limit p → ∞ we are left with kc(t)−c∗kL<sup>∞</sup> ≤ δ, which implies c(t, x) ∈ [c∗−δ, c∗+δ] = [c, c] as desired.

### 4.2 A density estimate for one-step minimizers

We will first prove a lemma that works for both spaces and their respective minimization movement schemes

$$
\mu_1 = \inf_{\mu \in \mathcal{M}(X)} \left\{ \frac{\mathsf{H} \mathsf{K}^2(\mu_0, \mu)}{2\tau} + \mathsf{E}(\mu) \right\}, \qquad \mu_1 = \inf_{\mu \in \mathcal{P}(X)} \left\{ \frac{\mathsf{H} \mathsf{K}^2(\mu_0, \mu)}{2\tau} + \mathsf{E}(\mu) \right\}, \tag{4.2}
$$

under the sole assumption that the function E, which generates E in (1.1), is convex. We will write (4.2)HK and (4.2)SHK to distinguish the two different incremental minimization schemes.

Note that for our bounded and convex domains <sup>X</sup> <sup>⊂</sup> **<sup>R</sup>** d the convergence with respect to HK or SHK is equivalent to the weak\* convergence of measures. Moreover, by our assumptions E is weakly\* lower semicontinuous and linearly bounded from below. Hence in both cases the functionals to be minimized are coercive (for τ > 0 sufficiently small) and weakly\* lower semicontinuous. Hence, the minimizing movement scheme is well defined for arbitrarily many steps.

We will start by providing a lemma that is a generalization of Otto's argument in [Ott96, Lem. 1.1.2]. Let µ<sup>1</sup> be defined as the one-step solution to either of the minimization schemes (4.2). According to the Lemma, mass can be transferred from a set A to some set T(A), only if it results in a situation where the density ρ<sup>1</sup> = dµ<sup>1</sup> <sup>d</sup>L<sup>d</sup> at the destination T(A), is less than the density in the original A. In the case of the Wasserstein distance where mass can only be transported, this is enough to prove bounds for ρ1, by studying the set where the density is below the minimum of ρ0. However, for our distances HK and SHK the arguments are a bit more involved.

Lemma 4.1 *Let* <sup>X</sup> <sup>⊂</sup> **<sup>R</sup>** d *be a compact, convex set with nonempty interior. Furthermore let* <sup>E</sup> *be as in* (1.1) *with* <sup>E</sup> *being a convex real-valued function. Let finally* <sup>µ</sup><sup>0</sup> <sup>≃</sup> <sup>L</sup> d , *and* µ<sup>1</sup> ∈ M(X) *as in* (4.2)HK *or* (4.2)SHK. *Then, either* µ<sup>1</sup> ≡ 0 *or* µ<sup>1</sup> ≃ L <sup>d</sup> *and, in both cases* HK *and* SHK*, the injective optimal transport-growth couple* (q, T) *from* µ<sup>0</sup> *to* µ<sup>1</sup> *provided by Theorem 3.6 satisfies*

$$
\rho_1(\mathbf{T}(x)) \le \rho_1(x) \quad \text{(alternatively } \rho_1(\mathbf{T}^{-1}(x)) \ge \rho_1(x) \text{)}, \quad \text{almost everywhere.} \tag{4.3}
$$

Proof. We are going to prove the statement in two parts. First we are going to show that µ<sup>1</sup> ≪ L d , with ρ1(T(x)) ≤ ρ1(x). On the second part we are going to prove L <sup>d</sup> ≪ µ1.

*Step A: Proving* µ<sup>1</sup> ≪ L <sup>d</sup> *and* ρ1(T(x)) ≤ ρ1(x). *Constructing the counterexample.*

We note that since µ<sup>0</sup> ≃ L d , it exits a transport-growth couple from µ<sup>0</sup> to µ1. Let µ s denote the singular part of µ in the Lebesgue decomposition. In case µ s 1 6≡ 0, it exists set B such that µ s 1 (B) 6= 0, and L d (B) = 0. The identity

$$
0 < \mu_1(B) = \int_B \mu_1(\mathrm{d}x) = \int_{T^{-1}(B)} q^2(x) \mu_0(\mathrm{d}x),\tag{4.4}
$$

guarantees that for the set A = T −1 (B) we have that µ0(A) 6= 0 and therefore L d (A) > 0. Having in mind to generate a contradiction to the assumption, we get that in both cases where either µ s 1 6≡ 0, or µ s <sup>1</sup> ≡ 0, if the assumption is violated then there exists a < b and a set A with L d (A) > 0 such that ρ1(x) < a < b < ρ1(T(x)) for every x ∈ A. When µ s 1 (T(A)) 6= 0, while L d (T(A)) = 0, we set ρ1(T(x)) equal to ∞. We define µ T(A) 1 to be the restriction of µ<sup>1</sup> onto T(A), and µ A 0 the restriction of µ<sup>0</sup> onto A. For 0 < t < 1, we define the measure µ t <sup>1</sup> <sup>=</sup> <sup>µ</sup><sup>1</sup> <sup>−</sup> tµ<sup>T</sup>(A) <sup>1</sup> + t µ T (A) 1 (X) µ<sup>A</sup> 0 (X) µ A 0 , which satisfies µ t 1 (X) = µ1(X). Moreover, by assumption we have A ∩ T(A) = ∅ and can decompose µ t 1 as

$$
\mu_1^t = \mu_1|_{X \setminus (A \cap T(A))} + (1-t)\mu_1|_{T(A)} + \left(\mu_1|_A + t\frac{\mu_1(T(A))}{\mu_0(A)}\mu_0|_A\right).
$$

Thus, nothing is changed on X \ (A ∩ T(A)), while mass is taken away on T(A) proportional to µ T(A) <sup>1</sup> = µ1|<sup>T</sup>(A) and added on A proportional to µ A <sup>0</sup> = µ0|A.

We will prove that the HK distance between µ<sup>0</sup> and µ<sup>1</sup> is not smaller than the resulting cost for this new transport-growth couple and therefore not smaller than HK(µ0, µ<sup>t</sup> 1 ). At the same time, we will show that E(µ t 1 ) < E(µ1), for small enough t, leading this way to a contradiction.

*Step A.1: Proving that the constructed measure is closer to* µ0*.* By applying Lemma 3.8 for A and X \ A, we obtain By applying Lemma 3.8 for A and X \ A, we obtain

$$
H^{2}(\mu_{0},\mu_{1}) = H^{2}(\mu_{0} - \mu_{0}^{A}, \mu_{1} - \mu_{1}^{T(A)}) + H^{2}(\mu_{0}^{A}, \mu_{1}^{T(A)})
$$
  
\n
$$
= H^{2}(\mu_{0} - \mu_{0}^{A}, \mu_{1} - \mu_{1}^{T(A)}) + H^{2}(\mu_{0}^{A}, (1-t)\mu_{1}^{T(A)} + t\frac{\mu_{1}^{T(A)}(X)}{\mu_{0}^{A}(X)}\mu_{0}^{A})
$$
  
\n
$$
+ H^{2}(\mu_{0}^{A}, \mu_{1}^{T(A)}) - H^{2}(\mu_{0}^{A}, (1-t)\mu_{1}^{T(A)} + t\frac{\mu_{1}^{T(A)}(X)}{\mu_{0}^{A}(X)}\mu_{0}^{A})
$$
  
\nsubadd.  
\n
$$
\geq H^{2}(\mu_{0}, \mu_{1}^{t}) + H^{2}(\mu_{0}^{A}, \mu_{1}^{T(A)}) - H^{2}(\mu_{0}^{A}, (1-t)\mu_{1}^{T(A)} + t\frac{\mu_{1}^{T(A)}(X)}{\mu_{0}^{A}(X)}\mu_{0}^{A})
$$
  
\nsubadd.  
\n
$$
\geq H^{2}(\mu_{0}, \mu_{1}^{t}) + H^{2}(\mu_{0}^{A}, \mu_{1}^{T(A)}) - H^{2}((1-t)\mu_{0}^{A}, (1-t)\mu_{1}^{T(A)}) - H^{2}(\mu_{0}^{A}, t\frac{\mu_{1}^{T(A)}(X)}{\mu_{0}^{A}(X)}\mu_{0}^{A})
$$
  
\n
$$
\geq H^{2}(\mu_{0}, \mu_{1}^{t}) + t(\mu_{0}^{2}(\mu_{0}^{A}, \mu_{1}^{T(A)}) - H^{2}(\mu_{0}^{A}, \frac{\mu_{1}^{T(A)}(X)}{\mu_{0}^{A}(X)}\mu_{0}^{A})) \geq H^{2}(\mu_{0}, \mu_{1}^{t}).
$$

(See [LMS18, Lem. 7.8] for the subadditivity of HK<sup>2</sup> .) In the last estimate we used that starting from a measure µ0, then among all measures with a given mass M, the measure Mµ<sup>0</sup> has the least distance. Indeed, we have

$$
\mathsf{H}^2 \left( \mu_0^A, \frac{\mu_1^{\mathbf{T}(A)}(X)}{\mu_0^A(X)} \mu_0^A \right) \stackrel{(3.12)}{=} \sqrt{\frac{\mu_1^{\mathbf{T}(A)}(X)}{\mu_0^A(X)}} \mathsf{H}^2(\mu_0^A, \mu_0^A) + \left( \frac{\mu_1^{\mathbf{T}(A)}(X)}{\mu_0^A(X)} - \sqrt{\frac{\mu_1^{\mathbf{T}(A)}(X)}{\mu_0^A(X)}} \right) \mu_0^A(X) + \left( 1 - \sqrt{\frac{\mu_1^{\mathbf{T}(A)}(X)}{\mu_0^A(X)}} \right) \mu_0^A(X) = \left( \sqrt{\mu_1^{\mathbf{T}(A)}(X)} - \sqrt{\mu_0^A(X)} \right)^2 \stackrel{(3.13)}{\leq} \mathsf{H}^2(\mu_0^A, \mu_1^{\mathbf{T}(A)})
$$

We will treat the case where µ<sup>1</sup> has a singular part and the case µ<sup>1</sup> ≪ L d separately.

*Step A.2.1: Entropy functional estimate; the case* µ <sup>s</sup> ≡ 0

$$
\mathsf{E}(\mu_1^t) - \mathsf{E}(\mu_1) = \int_X \left( E(\rho_1^t(x)) - E(\rho_1(x)) \right) \mathcal{L}^d(\mathrm{d}x)
$$
  
\n
$$
\leq \int_X E'(\rho_1^t(x)) \left( \rho_1^t(x) - \rho_1(x) \right) \mathcal{L}^d(\mathrm{d}x)
$$
  
\n
$$
= \int_X \left( E'(\rho_1^t(x) - E' \left( \frac{a+b}{2} \right)) \left( \rho_1^t(x) - \rho_1(x) \right) \mathcal{L}^d(\mathrm{d}x) \right),
$$
\n(4.5)

where in the last term the constant E′ ( a+b 2 ) could be inserted because of R X ρ <sup>t</sup>d L <sup>d</sup> = µ t (X) = µ1(X) = R X ρ1dL d . The integrand in the last term of (4.5) can be estimated as follows:

$$
(E'(\rho_1^t(x)) - E'(\frac{a+b}{2})) (\rho_1^t(x) - \rho_1(x))
$$
  
=  $t \Big( E'(\rho_1(x) + t \frac{\mu_1^{T(A)}(X)}{\mu_0^A(X)} \rho_0^A(x) - t \rho_1^{T(A)}(x) \Big) - E'(\frac{a+b}{2}) \Big) \Big( \frac{\mu_1^{T(A)}(X)}{\mu_0^A(X)} \rho_0^A(x) - \rho_1^{T(A)}(x) \Big)$   
 $\leq \begin{cases} t \Big( E'(a + t \frac{\mu_1^{T(A)}(X)}{\mu_0^A(X)} \rho_0(x)) - E'(\frac{a+b}{2}) \Big) \frac{\mu_1^{T(A)}(X)}{\mu_0^A(X)} \rho_0(x) & \text{on } A, \\ t \Big( E'(\frac{a+b}{2}) - E' (b - t \rho_1(x)) \Big) (\rho_1(x)) & \text{on } T(A). \end{cases}$ 

In both cases the factor multiplying t is negative for very small values of t. Hence, we have shown E(µ t ) < E(µ1) which is the desired contradiction.

*Step A.2.2: Entropy functional estimate; the case* µ s 1 6≡ 0. Let B such that L d (B) = 0 and µ s 1 (B<sup>c</sup> ) = 0

$$
E(\mu_1^t) - E(\mu_1) = \int_B \left( E(\rho_1^t(x)) - E(\rho_1(x)) \right) \mathcal{L}^d(\mathrm{d}x) + E'_{\infty} \mu_1^t(B^c) - E'_{\infty} \mu_1(B^c)
$$
  
\n
$$
\leq \int_B E'(\rho_1^t(x)) \left( \rho_1^t(x) - \rho_1(x) \right) \mathcal{L}^d(\mathrm{d}x) + E'_{\infty} \mu_1^t(B^c) - E'_{\infty} \mu_1(B^c)
$$
  
\n
$$
= \int_B \left( E'(\rho_1^t(x)) - E' \left( \frac{a+b}{2} \right) \right) (\rho_1^t(x) - \rho_1(x)) \mathcal{L}^d(\mathrm{d}x)
$$
  
\n
$$
+ \left( E'_{\infty} - E' \left( \frac{a+b}{2} \right) \right) (\mu_1^t(B^c) - \mu_1(B^c)), \tag{4.6}
$$

where in the last term the constant E′ ( a+b 2 ) could be inserted because of µ t (X) = µ1(X). The integral in the last term of (4.5) can be estimated as follows:

$$
\int_{B} \left( E'(\rho_1^t(x)) - E' \left( \frac{a+b}{2} \right) \right) (\rho_1^t(x) - \rho_1(x)) \mathcal{L}^d(\mathrm{d}x) \n\leq \int_{A} \left( E'(\rho_1^t(x)) - E' \left( \frac{a+b}{2} \right) \right) (\rho_1^t(x) - \rho_1(x)) \mathcal{L}^d(\mathrm{d}x) \n\leq \int_{A} t \left( E' \left( a + t \frac{\mu_1^{T(A)}(X)}{\mu_0^A(X)} \rho_0(x) \right) - E' \left( \frac{a+b}{2} \right) \right) \frac{\mu_1^{T(A)}(X)}{\mu_0^A(X)} \rho_0(x),
$$
\n(4.7)

while

$$
\mu_1^t(B^c) - \mu_1(B^c) = (\mu_1^t(\mathbf{T}(A)) - \mu_1(\mathbf{T}(A))) = t\left(E'\left(\frac{a+b}{2}\right) - E'_\infty\right)\mu_1(\mathbf{T}(A)) \quad (4.8)
$$

In both cases the factor multiplying t are negative for very small values of t. Hence, we have shown E(µ t ) < E(µ1) which is the desired contradiction.

*Step B: Proving* µ<sup>1</sup> ≡ 0 *or* L <sup>d</sup> ≪ µ1*.* We will assume that µ<sup>1</sup> 6≡ 0 and that there exists B = {x : ρ1(x) = 0} with L d (B) > 0 to reach a contradiction.

Since L d (B) > 0, there exist x<sup>0</sup> ∈ X and r<sup>0</sup> ∈ (0, π 4 ) such that L d (B(x0, r0) ∩ B) > 0 and B(x0, r0) ⊂ X. By the assumption µ<sup>1</sup> 6≡ 0, the set B<sup>c</sup> = X \ B satisfies L d (B<sup>c</sup> ) > 0, and therefore there exist x<sup>1</sup> ∈ X and r<sup>1</sup> ∈ (0, π 4 ) with L p (B(x1, r1) ∩ B<sup>c</sup> ) > 0 and B(x1, r1) ⊂ X.

We set r<sup>θ</sup> = (1−θ)r<sup>0</sup> + θr<sup>1</sup> and x<sup>θ</sup> = (1−θ)x<sup>0</sup> + θx<sup>1</sup> and observe that the convexity of <sup>X</sup> implies <sup>B</sup>(xθ, rθ) <sup>⊂</sup> **<sup>R</sup>** d for θ ∈ [0, 1]. As µ<sup>1</sup> is absolutely continuous the functions β(θ) := L d (B(xθ, rθ) ∩ B) and γ(θ) := L d (B(xθ, rθ) ∩ B<sup>c</sup> ) are continuous and satisfy β(θ) + γ(θ) = cdr d <sup>θ</sup> > 0 for all θ ∈ [0, 1]. With β(0) > 0 and γ(1) > 0 we conclude that there exists θ ∈ [0, 1] such that L d (B(xθ, rθ) ∩ B) > 0 and L d (B(xθ, rθ) ∩ B<sup>c</sup> ) > 0. For economy of notation, we denote B<sup>0</sup> = B(xθ, rθ) ∩ B and B<sup>1</sup> = B(xθ, rθ) ∩ B<sup>c</sup> .

From the assumption that µ<sup>0</sup> ≃ L d , we have µ0(B0) > 0. Also by definition of B<sup>c</sup> and therefore of B1, we have µ1(B1) > 0. Furthermore it holds that supx∈B0,y∈B<sup>1</sup> |x−y| < 2r<sup>θ</sup> < π <sup>2</sup> which means that µ<sup>1</sup> has positive value in a set that has distance less than π/2 from points in B0. More specifically, it holds B<sup>0</sup> ⊂ supp(µ ′ 0 ), and B<sup>1</sup> ⊂ supp(µ ′ 1 ), where µ ′ 0 and µ ′ 1 are as in Definition 3.4.

However, the optimality conditions in [LMS23, Cor. 3.5, (3.31)] provide |T(x)−x| < π/2 and σ0(x)σ1(T(x)) = cos<sup>2</sup> (|x−T(x)|) on B<sup>0</sup> ⊂ supp(µ ′ 0 ) which means that mass in B<sup>0</sup> could not be destroyed, but instead it must be transferred from B<sup>0</sup> somewhere in B<sup>c</sup> . This implies that T(x) ∈ B<sup>c</sup> for all x ∈ B0. However in Step A1, we proved ρ1(T(x)) ≤ ρ1(x) almost surely, therefore

$$
0 < \rho_1(\mathbf{T}(x)) \le \rho_1(x) = 0 \quad \text{for almost every} \quad x \in B_0.
$$

This gives us

$$
0 < \int_{B_0} \rho_1(\mathbf{T}(x)) \mathcal{L}^d(\mathrm{d}x) \le \int_{B_0} \rho_1(x) \mathcal{L}^d(\mathrm{d}x) = \mu_1(B_0) = 0
$$

which is a contradiction.

*The* SHK *case:* The proof for SHK is exactly the same, because the constructed counterexample has the same mass as the original and because the distance SHK is a strictly monotone function of the HK distance, namely SHK(µ0, µ1) = 2 arcsin 1 <sup>2</sup> HK(µ0, µ1) .

# 4.3 A single minimization step for HK

We proceed with proving upper and lower bounds for the density ρ<sup>1</sup> of the measure µ<sup>1</sup> defined by (4.2)HK, given that µ<sup>0</sup> satisfies some density bounds of its own, i.e. cmin < ρ<sup>0</sup> < cmax. The upper bound we retrieve is the same as in (D3), and the proof is relatively straightforward. We assume that the density ρ<sup>1</sup> of the minimizer µ<sup>1</sup> in (4.2)HK is bigger than the expected density given by (D3) in a set of positive Lebesgue measure. By applying Lemma 4.1, we infer excessive creation of mass in some set B. We conclude that, what we "gain" in HK distance by restricting the growth on B is more than what we "lose" for the entropy function. Therefore ending up with a measure that has total energy less than µ1, which is a contradiction. A similar bound was retrieved in [DiC20] for a class of Entropy functionals that were studied in that paper.

Proposition 4.2 (Upper bound for incremental densities for HK) *Let* <sup>X</sup> <sup>⊂</sup> **<sup>R</sup>** d *be a compact, convex set with nonempty interior. Furthermore consider* E *as in* (1.1) *with* E *being convex. Let finally* µ<sup>0</sup> = ρ<sup>0</sup> L <sup>d</sup> *with* ρ0(x) ≤ cmax *and* µ<sup>1</sup> ∈ M(X) *as in* (4.2)HK*. Then, for all* cupp ≥ 0 *and* τ > 0 *with* E′ (cupp)τ > −1/2 *we have*

$$
\rho_1(x) \le \max\left\{c_{\text{upp}}, \frac{c_{\text{max}}}{(1 + 2\tau \min\{E'(c_{\text{upp}}), 0\})^2}\right\}.
$$
\n(4.9)

Proof. We start by setting

$$
k_{\epsilon} := \frac{1}{1 + 2\tau \min\{E'(c_{\text{upp}}), 0\}} + \epsilon \ge 1 + \epsilon.
$$

*Step 1: Construction of sets.* In order to arrive a a contradiction we define the set

$$
B := \left\{ x \in X \mid \rho_1(x) \ge \xi_{\epsilon} \right\} \quad \text{with } \xi_{\epsilon} := \max\{ (1+\epsilon)^2 c_{\text{upp}}, k_{\epsilon}^2 c_{\text{max}} \}
$$

and assume <sup>µ</sup>1(B) <sup>&</sup>gt; <sup>0</sup>. By Lemma 4.1, for almost every <sup>x</sup> <sup>∈</sup> <sup>T</sup>−<sup>1</sup> (B), we have

$$
\rho_1(\boldsymbol{T}^{-1}(x)) \ge \rho_1(x) \ge \xi_{\epsilon},
$$

which yields x ∈ B, and we conclude T −1 (B) ⊂ B. By using this we find

$$
\mu_1(B) = \int_B \rho_1(x) \mathcal{L}^d(\mathrm{d}x) \ge \int_{\mathbf{T}^{-1}(B)} \rho_1(x) \mathcal{L}^d(\mathrm{d}x)
$$
  
\n
$$
\ge \int_{\mathbf{T}^{-1}(B)} k_\epsilon^2 c_{\max} \mathcal{L}^d(\mathrm{d}x) \ge \int_{\mathbf{T}^{-1}(B)} k_\epsilon^2 \rho_0(x) \mathcal{L}^d(\mathrm{d}x) = k_\epsilon^2 \mu_0(\mathbf{T}^{-1}(B)),
$$

where in the last estimates we applied both ρ1(x) ≥ k 2 ǫ cmax and cmax ≥ ρ0(x). By combining the estimate above with the definition of the injective optimal transport couple we have,

$$
k_{\epsilon}^{2}\mu_{0}(\boldsymbol{T}^{-1}(B)) \leq \mu_{1}(B) = \int_{\boldsymbol{T}^{-1}(B)} q^{2}\mu_{0}(\mathrm{d}x),
$$

which leads to the existence of a set A ⊂ T −1 (B), for which µ0(A) > 0 and q 2 (x) ≥ k 2 ǫ for a.a. x ∈ A.

Step 2: Comparison of HK distances. We denote by µ A 0 the restriction of µ<sup>0</sup> on A, and by µ T(A) 1 the restriction of µ<sup>1</sup> on T(A). Since (T, tq) is a transport-growth couple for µ A 0 , t<sup>2</sup>µ T(A) 1 for t ∈ [0, 1], we get

$$
\mathsf{H}^{2}\left(\mu_{0}^{A}, t^{2}\mu_{1}^{T(A)}\right) \leq \int_{A} \left(1 + (tq(x))^{2} - 2tq(x)\cos\left(|T(x) - x|\right)\right)\mu_{0}^{A}(\mathrm{d}x),\tag{4.10}
$$

from which we obtain

$$
\begin{split} \mathsf{H}\mathsf{K}^{2}\left(\mu_{0}^{A},\mu_{1}^{T(A)}\right) &-\mathsf{H}\mathsf{K}^{2}\left(\mu_{0}^{A},t^{2}\mu_{1}^{T(A)}\right) \geq \int_{X} (1+(q(x))^{2}-2q(x)\cos\left(|T(x)-x|\right))\,\mu_{0}^{A}(\mathrm{d}x) \\ &-\int_{X} (1+(tq(x))^{2}-2tq(x)\cos\left(|T(x)-x|\right))\,\mu_{0}^{A}(\mathrm{d}x) \\ &=\int_{X} (1-t^{2})q^{2}(x)-2(1-t)q(x)\cos\left(|T(x)-x|\right)\mu_{0}^{A}(\mathrm{d}x) \\ &\geq \int_{X} \left[ (1-t^{2})q^{2}(x)-\frac{2(1-t)}{q(x)}q^{2}(x)\right]\,\mu_{0}^{A}(\mathrm{d}x) \geq (1-t)\left(1+t-\frac{2}{k_{\epsilon}}\right)\int_{X} q^{2}(x)\mu_{0}^{A}(\mathrm{d}x) \\ &=(1-t)\left(1+t-\frac{2}{k_{\epsilon}}\right)\mu_{1}^{T(A)}(X), \end{split} \tag{4.11}
$$

where in the last estimate, we applied q(x) ≥ k<sup>ǫ</sup> . Now, for 0 ≤ t ≤ 1, we define the measure

$$
\mu_1^t := \mu_1 - \mu_1^{T(A)} + t^2 \mu_1^{T(A)} = \mu_1 + (t^2 - 1)\mu_1^{T(A)} = (\mathbf{1}_{X \setminus T(A)} + t^2 \mathbf{1}_{T(A)}) \rho_1 \mathcal{L}^d.
$$

Applying Lemma 3.8 for A and X \ A, we get

$$
\begin{split} \mathsf{H}\mathsf{K}^{2}(\mu_{0},\mu_{1}) &= \mathsf{H}\mathsf{K}^{2}(\mu_{0} - \mu_{0}^{A}, \mu_{1} - \mu_{1}^{\mathbf{T}(A)}) + \mathsf{H}\mathsf{K}^{2}\left(\mu_{0}^{A}, \mu_{1}^{\mathbf{T}(A)}\right) \\ &\geq \mathsf{H}\mathsf{K}^{2}(\mu_{0} - \mu_{0}^{A}, \mu_{1} - \mu_{1}^{\mathbf{T}(A)}) + \mathsf{H}\mathsf{K}^{2}\left(\mu_{0}^{A}, t^{2} \mu_{1}^{\mathbf{T}(A)}\right) + \mathsf{H}\mathsf{K}^{2}\left(\mu_{0}^{A}, \mu_{1}^{\mathbf{T}(A)}\right) - \mathsf{H}\mathsf{K}^{2}\left(\mu_{0}^{A}, t^{2} \mu_{1}^{\mathbf{T}(A)}\right) \\ &\geq \mathsf{H}\mathsf{K}^{2}(\mu_{0}, \mu_{1}^{t}) + (1-t)\left(1 + t - \frac{2}{k_{\epsilon}}\right)\mu_{1}^{\mathbf{T}(A)}(X), \end{split} \tag{4.12}
$$

where the last inequality is a result of the sub-additivity of the squared distance and (4.11). So, for the new measure µ t <sup>1</sup> = µ<sup>1</sup> + (t <sup>2</sup>−1)µ T(A) 1 , we have

$$
\frac{1}{2\tau} \left( \mathsf{H}^2(\mu_0, \mu_1) - \mathsf{H}^2(\mu_0, \mu_1^t) \right) \ge \frac{1-t}{2\tau} \left( 1 + t - \frac{2}{k_\epsilon} \right) \mu_1^{\mathbf{T}(A)}(X). \tag{4.13}
$$

For later use we recall that

$$
\mu_1^{T(A)}(X) = \int_X q^2 \mu_0^A(\mathrm{d}x) = \int_A q^2 \mu_0(\mathrm{d}x) \ge \int_A k_\epsilon^2 \mu_0(\mathrm{d}x) \ge (1+\epsilon)^2 \mu(A) > 0. \tag{4.14}
$$

Step 3: Comparison of entropies. On T(A) we have ρ t <sup>1</sup> = t <sup>2</sup>ρ<sup>1</sup> ≥ t 2 ξǫ , and the convexity of E gives

$$
\mathsf{E}(\mu_1) - \mathsf{E}(\mu_1^t) = \int_{\mathbf{T}(A)} \left( E(\rho_1(x)) - E(\rho_1^t(x)) \right) \mathcal{L}^d(\mathrm{d}x)
$$
  
\n
$$
\geq \int_{\mathbf{T}(A)} E'(\rho_1^t)(\rho_1(x) - \rho_1^t(x)) \mathcal{L}^d(\mathrm{d}x) \geq E'(t^2 \xi_\epsilon) \int_{\mathbf{T}(A)} (\rho_1 - \rho_1^t) \mathcal{L}^d(\mathrm{d}x).
$$

For all t ∈ (1+ǫ) −1/2 , 1 we have t 2 ξ<sup>ǫ</sup> ≥ cupp, and therefore E ′ (t 2 ξǫ) ≥ E ′ (cupp) by the monotonicity of E′ . With this we arrive at the lower bound

$$
\mathsf{E}(\mu_1) - \mathsf{E}(\mu_1^t) \ge E'(c_{\text{upp}}) \int_{\mathbf{T}(A)} (\rho_1(x) - \rho_1^t(x)) \mathcal{L}^d(\text{d}x)
$$
\n
$$
= E'(c_{\text{upp}}) \int_{\mathbf{T}(A)} (1 - t^2) \rho_1(x) \mathcal{L}^d(\text{d}x) = (1 - t^2) E'(c_{\text{upp}}) \mu_1^{\mathbf{T}(A)}(X). \tag{4.15}
$$

Step 4: Minimization provides contradiction. Because µ<sup>1</sup> is a minimizer we reach a contradiction if we find a t ∈ (0, 1) such that

$$
\eta(t) := \frac{1}{2\tau} \mathsf{H}^2(\mu_0, \mu_1) + \mathsf{E}(\mu_1) - \left(\frac{1}{2\tau} \mathsf{H}^2(\mu_0, \mu_1^t) + \mathsf{E}(\mu_1^t)\right) > 0.
$$

Combining the estimates (4.13) and (4.15) we find, for t ∈ (1+ǫ) −1/2 , 1 , the lower estimate

$$
\eta(t) \ge \overline{\eta}(t)\mu_1^*(X) \quad \text{with } \overline{\eta}(t) := \frac{1-t}{2\tau} \left(1 + t - \frac{2}{k_{\epsilon}}\right) + (1 - t^2)E'(c_{\text{upp}}).
$$

Clearly, we have η(1) = 0 and find

$$
\overline{\eta}'(1) = -\frac{1}{\tau} \left( 1 + 2\tau E'(c_{\text{upp}}) - \frac{1}{k_{\epsilon}} \right) \leq -\frac{1}{\tau} \left( \frac{1}{k_{\epsilon} - \epsilon} - \frac{1}{k_{\epsilon}} \right) = -\frac{1}{\tau} \frac{\varepsilon}{(k_{\epsilon} - \epsilon)k_{\epsilon}} < 0.
$$

Recalling µ T(A) 1 (X) > 0 from (4.14) we obtain η(t) > 0 for all t < 1 that are sufficiently close to t = 1. Thus, the assumption µ(B) > 0 must have been false, and we conclude ρ1(x) ≤ ξ<sup>ǫ</sup> a.e. in X. As ǫ > 0 was arbitrary, the assertion is established.

We proceed with the lower bound.

Proposition 4.3 (Lower bound for incremental densities) *Let* <sup>X</sup> <sup>⊂</sup> **<sup>R</sup>** d *be a compact, convex set with nonempty interior. Furthermore let* E *be as in* (1.1) *with convex* E. *Let finally* µ<sup>0</sup> ≃ L <sup>d</sup> *with* ρ0(x) ≥ cmin, *and* µ<sup>1</sup> ∈ M(X) *as in* (4.2)HK*. Is it true that*

$$
\rho_1(x) \ge \min \left\{ c_{\text{low}}, \, \frac{c_{\min}}{(1 + 2\tau \max\{E'(c_{\text{low}}), 0\})^2} \right\} \quad \text{for all } c_{\text{low}} \ge 0 \tag{4.16}
$$

Proof. We start by setting

$$
k_{\epsilon} := \frac{1}{1 + 2\tau \, \max\{E'(c_{\text{low}}), 0\}} - \epsilon \le 1 - \epsilon.
$$

*Step 1: Construction of sets.* In order to arrive a a contradiction we define the set

$$
B := \left\{ x \in X \mid \rho_1(x) \le \xi_{\epsilon} \right\} \quad \text{with } \xi_{\epsilon} := \min\{ (1 - \epsilon)^2 c_{\text{low}}, k_{\epsilon}^2 c_{\text{min}} \}
$$

and assume µ1(B) > 0. By Lemma 4.1, for almost every x ∈ B, we have

$$
\rho_1(\mathbf{T}(x)) \le \rho_1(x) \le \xi_{\epsilon},
$$

which yields T(x) ∈ B, and we conclude T(B) ⊂ B. By using this we find

$$
\mu_1(\mathbf{T}(B)) = \int_{\mathbf{T}(B)} \rho_1(x) \mathcal{L}^d(\mathrm{d}x) \le \int_B \rho_1(x) \mathcal{L}^d(\mathrm{d}x)
$$
  
$$
\le \int_B k_\epsilon^2 c_{\min} \mathcal{L}^d(\mathrm{d}x) \le \int_B k_\epsilon^2 \rho_0(x) \mathcal{L}^d(\mathrm{d}x) = k_\epsilon^2 \mu_0(B),
$$

where in the last estimates we applied both ρ1(x) ≤ k 2 ǫ cmin and cmin ≤ ρ0(x). By combining the estimate above with the definition of the injective optimal transport couple we have,

$$
k_{\epsilon}^{2}\mu_{0}(B) \leq \mu_{1}(\boldsymbol{T}(B)) = \int_{B} q^{2}\mu_{0}(\mathrm{d}x),
$$

which leads to the existence of a set A ⊂ B, for which µ0(A) > 0 and q 2 (x) ≤ k 2 ǫ for a.e. x ∈ A. For this A, without loss of generality we can even assume that ρ0(x) < cmax, for some cmax > 0.

Step 2: Comparison of HK distances. We denote by µ A 0 the restriction of µ<sup>0</sup> on A, and by µ T(A) 1 the restriction of µ<sup>1</sup> on T(A). Since (T, q/t) is a transport-growth couple for µ A 0 ,(1/t<sup>2</sup> )µ T(A) 1 for t ∈ [0, 1], we get

$$
\mathsf{H}^{2}\left(\mu_{0}^{A}, \frac{1}{t^{2}}\mu_{1}^{T(A)}\right) \leq \int_{A}\left(1 + \left(\frac{q(x)}{t}\right)^{2} - 2\frac{q(x)}{t}\cos\left(|T(x) - x|\right)\right)\mu_{0}^{A}(\mathrm{d}x),\qquad(4.17)
$$

from which we obtain

$$
\begin{split}\n\mathsf{H}^{2}\left(\mu_{0}^{A},\mu_{1}^{T(A)}\right) - \mathsf{H}^{2}\left(t^{2}\mu_{0}^{A},\mu_{1}^{T(A)}\right) &\geq \mathsf{H}^{2}\left(\mu_{0}^{A},\mu_{1}^{T(A)}\right) - t^{2}\mathsf{H}^{2}\left(\mu_{0}^{A},\frac{\mu_{1}^{T(A)}}{t^{2}}\right) \geq \\
\int_{X}\left(1 + (q(x))^{2} - 2q(x)\cos\left(|T(x) - x|\right)\right)\mu_{0}^{A}(\mathrm{d}x) \\
&\quad - \int_{X}\left(t^{2} + q(x)^{2} - 2tq(x)\cos\left(|T(x) - x|\right)\mu_{0}^{A}(\mathrm{d}x)\right) \\
&\geq \int_{X}\left(1 - t^{2}\right) - 2(1 - t)q(x)\cos\left(|T(x) - x|\right)\mu_{0}^{A}(\mathrm{d}x) \\
&\geq \int_{X}\left[(1 - t^{2}) - 2(1 - t)q(x)\right]\mu_{0}^{A}(\mathrm{d}x) \geq (1 - t)\left(1 + t - 2k_{\epsilon}\right)\int_{X}\mu_{0}^{A}(\mathrm{d}x) \\
&= (1 - t)\left(1 + t - 2k_{\epsilon}\right)\mu_{0}^{A}(X),\n\end{split}
$$

where in the last estimate, we applied q(x) ≤ k<sup>ǫ</sup> . Now, for 0 ≤ t ≤ 1, we define the measure

$$
\mu_1^t := \mu_1 + (1 - t^2)\mu_0^A.
$$

Applying Lemma 3.8 for A and X \ A, we get

$$
\begin{split}\n\mathsf{H}^{2}(\mu_{0},\mu_{1}) &= \mathsf{H}^{2}(\mu_{0} - \mu_{0}^{A}, \mu_{1} - \mu_{1}^{\mathbf{T}(A)}) + \mathsf{H}^{2}\left(\mu_{0}^{A}, \mu_{1}^{\mathbf{T}(A)}\right) \\
&= \mathsf{H}^{2}(\mu_{0} - \mu_{0}^{A}, \mu_{1} - \mu_{1}^{\mathbf{T}(A)}) + \mathsf{H}^{2}\left(\mu_{0}^{A}, (1-t^{2})\mu_{0}^{A} + \mu_{1}^{\mathbf{T}(A)}\right) \\
&+ \mathsf{H}^{2}\left(\mu_{0}^{A}, \mu_{1}^{\mathbf{T}(A)}\right) - \mathsf{H}^{2}\left((1-t^{2})\mu_{0}^{A} + t^{2}\mu_{0}^{A}, (1-t^{2})\mu_{0}^{A} + \mu_{1}^{\mathbf{T}(A)}\right) \\
&\geq \mathsf{H}^{2}(\mu_{0}, \mu_{1}^{t}) + \mathsf{H}^{2}\left(\mu_{0}^{A}, \mu_{1}^{\mathbf{T}(A)}\right) - \mathsf{H}^{2}\left((1-t^{2})\mu_{0}^{A}, (1-t^{2})\mu_{0}^{A}\right) - \mathsf{H}^{2}\left(t^{2}\mu_{0}^{A}, \mu_{1}^{\mathbf{T}(A)}\right) \\
&\geq \mathsf{H}^{2}(\mu_{0}, \mu_{1}^{t}) + (1-t)\left(1+t - 2k_{\epsilon}\right)\mu_{0}^{A}(X),\n\end{split} \tag{4.19}
$$

where the last inequality is a result of the sub-additivity of the squared distance and (4.18). So, for the new measure, we have

$$
\frac{1}{2\tau} \left( \mathsf{H}^2(\mu_0, \mu_1) - \mathsf{H}^2(\mu_0, \mu_1^t) \right) \ge \frac{1-t}{2\tau} \left( 1 + t - 2k_\epsilon \right) \mu_0^A(X). \tag{4.20}
$$

Step 3: Comparison of entropies. On A we have ρ t <sup>1</sup> = ρ1+(1−t 2 )ρ0(x) ≤ ξǫ+(1−t 2 )cmax, and the convexity of E gives

$$
\mathsf{E}(\mu_1^t) - \mathsf{E}(\mu_1) = \int_A \left( E(\rho_1^t(x)) - E(\rho_1(x)) \right) \mathcal{L}^d(\mathrm{d}x) \n\leq \int_A E'(\rho_1^t) (\rho_1^t(x) - \rho_1(x)) \mathcal{L}^d(\mathrm{d}x) \leq E'(\xi_\epsilon + (1-t^2)c_{\text{max}}) \int_A (\rho_1^t - \rho_1) \mathcal{L}^d(\mathrm{d}x).
$$

For all t ∈ (1+ǫ/cmax) −1/2 , 1 we have ξ<sup>ǫ</sup> + (1−t 2 )cmax ≤ clow, and therefore E′ (ξ<sup>ǫ</sup> + (1−t 2 )cmax) ≤ E′ (clow) by the monotonicity of E′ . With this we arrive at the lower bound

$$
\mathsf{E}(\mu_1^t) - \mathsf{E}(\mu_1) \le E'(c_{\text{low}}) \int_A (\rho_1^t(x) - \rho_1(x)) \mathcal{L}^d(\mathrm{d}x) \n= E'(c_{\text{low}}) \int_A (1 - t^2) \rho_0(x) \mathcal{L}^d(\mathrm{d}x) = (1 - t^2) E'(c_{\text{low}}) \mu_0^A(X).
$$
\n(4.21)

Step 4: Minimization provides contradiction. Because µ<sup>1</sup> is a minimizer we reach a contradiction if we find a t ∈ (0, 1) such that

$$
\eta(t) := \frac{1}{2\tau} \mathsf{H}^2(\mu_0, \mu_1) + \mathsf{E}(\mu_1) - \left(\frac{1}{2\tau} \mathsf{H}^2(\mu_0, \mu_1^t) + \mathsf{E}(\mu_1^t)\right) > 0.
$$

Combining the estimates (4.13) and (4.15) we find, for t ∈ (1+ǫ/cmax) −1/2 , 1 , the lower estimate

$$
\eta(t) \ge \overline{\eta}(t)\mu_0^A(X)
$$
 with  $\overline{\eta}(t) := \frac{1-t}{2\tau}(1+t-2k_{\epsilon}) - (1-t^2)E'(c_{\text{low}}).$ 

Clearly, we have η(1) = 0 and find

$$
\overline{\eta}'(1) = \frac{1}{\tau} \big( k_{\epsilon} - (1 - 2\tau E'(c_{\text{low}})) \big) \le \frac{1}{\tau} \left( k_{\epsilon} - \frac{1}{1 + 2\tau E'(c_{\text{low}})} \right) < 0.
$$

Recalling µ A 0 (X) > 0 from (4.14) we obtain η(t) > 0 for all t < 1 that are sufficiently close to t = 1. Thus, the assumption µ(B) > 0 must have been false, and we conclude ρ1(x) ≥ ξ<sup>ǫ</sup> a.e. in X. As ǫ > 0 was arbitrary, the assertion is established.

# 4.4 A single minimization step for SHK

We will proceed with the theorem for the spherical Hellinger Kantorovich describing the propagation of density bounds for the incremental minimization scheme for the gradient system (P(X), E, SHK).

The main argument for the spherical Hellinger Kantorovich goes as follows. If the assumption for either bounds of ρ<sup>1</sup> is violated, then the fact that the mass remains constant, guarantees the existence of two sets A, B of positive measure that will lead to a contradiction. More specifically, for mass leaving A, we have growth (i.e. q > 1), with a resulting density at the target bigger than some constant c. At the same time for B, we have that the final density is strictly smaller than c and part of the mass that left B was destroyed, (i.e. q < 1). One can show that it is cheaper to reduce the growth of the mass leaving A and going T(A) resulting in less density in T(A), where at the same time for the mass leaving B we retain a portion at place instead of destroying it during the transportation, which again leads to a cheaper cost. This way we can construct a new measure that contradicts the optimality of µ1.

Proposition 4.4 (Density bounds for SHK) *Let* <sup>X</sup> <sup>⊂</sup> **<sup>R</sup>** d *be a compact, convex set with nonempty interior. Furthermore let* E *be as in* (1.1) *with convex* E. *Let finally* µ<sup>0</sup> = ρL d *with* cmin ≤ ρ0(x) ≤ cmax, *and* µ<sup>1</sup> ∈ P(X) *as in* (4.2)SHK*. Then, we have the density bounds*

$$
c_{\min} \le \rho_1(x) \le c_{\max} \quad almost \; everywhere \; in \; X. \tag{4.22}
$$

Proof. Step 1: Description of proof strategy. Let (q, T) be an optimal transport-growth couple from µ<sup>0</sup> to µ1. By Lemma 4.1 we have that µ<sup>1</sup> ≃ L d . We remind the reader that in this case, T is injective with no loss of generality. We will prove that if the lower or upper bound in equation (4.22) is violated in a set of positive measure, then there exist sets A and B of positive Lebesgue measure and a positive constants ǫ such that

S.1 L(A) > 0 and ∀ x ∈ A : q 2 (x) > 1+ǫ and ρ1(T(x)) > (1+ǫ)c.

**S.2** 
$$
\mathcal{L}(B) > 0
$$
 and  $\forall x \in B: q^2(x) < 1-\epsilon$  and  $\rho_1(x) < (1-\epsilon)\overline{c}$ ,

where c = cmin if the lower bound is violated and c = cmax if the upper bound is violated.

Based on this, we can then construct a new measure µ t,s <sup>1</sup> with unit mass, but with lower density in T(A) and higher density in B, resulting in a lower value in the minimizing scheme than µ1. This will contradict the assumption that µ<sup>1</sup> is a minimizer.

Step 2: Construction of A and B if the lower bound is violated. We define the sets

$$
X_{\leq c} := \{ x \in X \mid \rho_1(x) \leq c \} \text{ and } X_{>c} := \{ x \in X \mid \rho_1(x) > c \},
$$

such that X = X≤<sup>c</sup> . ∪X>0. Throughout, we fix a measurable representative for the density ρ1. The violation of the lower bound meant that

$$
\exists c_1 < c_{\min} : \mathcal{L}^d(X_{\leq c_1}) > 0.
$$

Step 2.1: Construction of B. By Lemma 4.1, we have ρ1(T(x)) ≤ ρ1(x) a.e. on X. Hence for all c > 0 we find

$$
x \in X_{\leq c} \implies \rho_1(x) \leq c \implies \rho_1(\mathbf{T}(x)) \leq c \implies \mathbf{T}(x) \in X_{\leq c}
$$
 a.e. on X.

This implies L d (T(X≤c)) ≤ L d (X≤c), which in turn implies that

$$
\mu_1(\mathbf{T}(X_{\leq c})) = \int_{\mathbf{T}(X_{\leq c})} \rho_1(y) \mathcal{L}^d(\mathrm{d}y) \leq c \mathcal{L}^d(\mathbf{T}(X_{\leq c})) \leq c \mathcal{L}^d(X_{\leq c}) \leq \frac{c}{c_{\min}} \mu_0(X_{\leq c_{\min}}),
$$

since ρ0(x) ≥ cmin a.e. on X. However, by the definition of the transport-growth couple (T, q), we have µ1(T(X≤c)) = R X≤<sup>c</sup> q <sup>2</sup>µ0(dx). Hence for c ∈ [c1, cmin] there must be a set B<sup>c</sup> ⊂ X≤<sup>c</sup> with L(Bc) > 0 on which q <sup>2</sup> ≤ c/cmin a.e. Therefore, S.2 is satisfied for all ǫ ∈ ]0, 1−c1/cmin[.

Step 2.2: Construction of A. In Step 2.1 we showed that T(X≤c) ⊂ X≤<sup>c</sup> ∪ N for a null set N. Using that M<sup>0</sup> ⊂ M<sup>1</sup> implies T −1 (M0) ⊂ T −1 (M1) and that T is injective, we find

$$
X_{\leq c} \stackrel{\text{inject}}{=} \boldsymbol{T}^{-1}(\boldsymbol{T}(X_{\leq c})) \subset \boldsymbol{T}^{-1}\big(X_{\leq c} \cup N\big) = \boldsymbol{T}^{-1}(X_{\leq c}) \cup \widetilde{N}.
$$

This implies L d (X≤c) ≤ L d (T −1 (X≤c)), and as in Step 2.1 we obtain

$$
\mu_1(X_{\leq c}) = \int_{X_{\leq c}} \rho_1(y) \mathcal{L}^d(\mathrm{d}y) \leq c \mathcal{L}^d(X_{\leq a}) \leq c \mathcal{L}^d(\mathbf{T}^{-1}(X_{\leq c})) \leq \frac{c}{c_{\min}} \mu_0(\mathbf{T}^{-1}(X_{\leq c})).
$$
\n(4.23)

We claim that there is a c<sup>2</sup> > cmin such that

$$
m_1(c_2) := \mu_1(X_{\leq c_2}) \leq \frac{1}{2} (m_1(c_{\min}) + m_0(c_{\min})) < m_0(c_{\min}), \tag{4.24}
$$

where m0(c) := µ0(T −1 (X≤c)). For this we first observe

$$
\mu_1(X_{\leq c_{\min}}) \leq c_{\min} \mathcal{L}^d\big(X_{\leq c_{\min}} \setminus X_{c_1}\big) + c_1 \mathcal{L}^d(X_{\leq c_1}) < c_{\min} \mathcal{L}^d(X_{\leq c_{\min}}).
$$

This implies that (4.23) holds for c = cmin with strict inequality, i.e. m1(cmin) < m0(cmin). We set δ := (m0(cmin)−m1(cmin))/2 > 0.

Next, we observe that m<sup>0</sup> and m<sup>1</sup> are non-decreasing functions by definition of X≤a. Finally, m<sup>1</sup> (and also m0) is continuous from the right, because a<sup>k</sup> ↓ a implies X≤<sup>a</sup> = ∩ ∞ <sup>k</sup>=1X≤a<sup>k</sup> and the measure µ<sup>1</sup> is continuous along non-increasing sequences. With this, we find c<sup>2</sup> > cmin such that m1(c2) ≤ m1(cmin)+δ = m0(cmin)−δ < m0(cmin), and (4.24) is established.

Switching to X>a = X \ X≤<sup>a</sup> and using µ<sup>j</sup> (X>a) = 1 − µ<sup>j</sup> (X≤a) we find, for all c ∈ [cmin, c3] with c<sup>3</sup> = min{c2,(1+δ)cmin}, the estimate

$$
\mu_1(X_{>c}) = 1 - m_1(c) \ge 1 - m_1(c_2) \ge 1 - m_2(c_{\min}) + \delta
$$
  
=  $\mu_0(X_{>c_{\min}}) + \delta \ge \mu_0(X_{>c}) + (\frac{c}{c_{\min}} - 1) \ge \frac{c}{c_{\min}} \mu_0(X_{>c_{\min}}).$ 

By the definition of (T, q) we also have µ1(X>c) = R T <sup>−</sup>1(X>c) q <sup>2</sup>µ0(dy), which implies, for all c ∈ ]cmin, c3[, the existence of a measurable set A<sup>c</sup> ⊂ X>c with L d (Ac) > 0 and q <sup>2</sup> > c/cmin a.e. in Ac. Hence, S.1 is satisfied for all ǫ ∈ (c3−cmin)/cmin.

Step 3.0: Construction of A and B if the upper bound is violated. We proceed completely analogous to Step 2 by interchanging the inequality signs and replacing T by T −1 . The violation of the upper bound means that

$$
\exists c_4 > c_{\max} : \mathcal{L}^d(X_{\geq c_4}) > 0,
$$

where now X≥<sup>c</sup> = x ∈ X  ρ1(x) ≥ c and X<c = x ∈ X <sup>ρ</sup>1(x) < c .

Step 3.1: Construction of the set A. Lemma 4.1 provides ρ1(T −1 (x)) ≥ ρ1(x) a.e. on X, and hence L d (X≥c) ≥ L d (T −1 (X≥c)). As in Step 2.1 we have

$$
\int_{\mathbf{T}^{-1}(X_{\geq c})} q^2 \mu_0(\mathrm{d}x) = \mu_1(X_{\geq c}) \geq c \mathcal{L}^d(X_{\geq c}) \geq c \mathcal{L}^d(\mathbf{T}^{-1}(X_{\geq c})) \geq \frac{c}{c_{\max}} \mu_0(\mathcal{L}^d(\mathbf{T}^{-1}(X_{\geq c}))).
$$

Hence, for all c ∈ ]cmax, c4] there exists A<sup>c</sup> ⊂ X≥<sup>c</sup> such that q <sup>2</sup> ≥ c/cmax a.e. on Ac, i.e. S.1 holds for all ǫ ∈ ]0,(c4−cmax)/cmax[.

Step 3.2: Construction of the set B. Using Lemma 4.1 and the injectivity of T we obtain L d (T(X≥c)) ≥ L d (X≥c) and furthermore

$$
M_1(c) := \mu_1(\mathbf{T}(X_{\ge c})) \ge c\mathcal{L}^d(\mathbf{T}(X_{\ge c})) \ge c\mathcal{L}^d(X_{\ge c}) \ge \frac{cM_0(c)}{c_{\text{max}}} \text{ with } M_0(c) := \mu_0(X_{\ge c}).
$$

Using 0 < M0(c4) = L d (X≥c<sup>4</sup> ) with c<sup>4</sup> > cmax we easily see M1(cmax) > M0(cmax).

Using the M<sup>0</sup> and M<sup>1</sup> are non-increasing in c and that M<sup>1</sup> is continuous from the left, we can argue as in Step 2.2 to find c<sup>5</sup> < cmax such that for c ∈ ]c5, cmax[ we have

$$
\int_{X_{< c}} q^2 \mu_0(\mathrm{d}y) = \int_{\mathbf{T}(X_{< c})} \mu_1(\mathrm{d}y) = \mu_1(\mathbf{T}(X_{< c})) = 1 - M_1(c) \le \frac{c(1 - M_0(c))}{c_{\text{max}}} = \frac{c\mu_0(X_{< c})}{c_{\text{max}}}.
$$

Thus, there exists B<sup>c</sup> ⊂ X<c with L d (X<c) > 0 and q <sup>2</sup> ≤ c/cmax a.e. on X<c, and S.2 holds for all ǫ ∈ ]0,(cmax−c5)/cmax[.

Step 4: Construction of a counterexample. We start by defining µ A 0 the restriction of <sup>µ</sup><sup>0</sup> on A, µ<sup>T</sup>(A) 1 the restriction of µ<sup>1</sup> on T(A), µ<sup>B</sup> 0 the restriction of µ<sup>0</sup> on B, and µ T(B) 1 the restriction of µ<sup>1</sup> on T(B). We define the comparison measure by

$$
= \mu_1 - s\mu_1^{T(A)} + t\mu_0^B
$$

where small s > 0 and t > 0 are chosen later.

Applying the splitting from Lemma 3.8 to (T, q) we obtain

$$
\mathsf{H}^{2}(\mu_{0}, \mu_{1}) = \mathsf{H}^{2}(\mu_{0} - \mu_{0}^{A} - \mu_{0}^{B}, \mu_{1} - \mu_{1}^{T(A)} - \mu_{1}^{T(B)}) + \mathsf{H}^{2}(\mu_{0}^{A}, \mu_{1}^{T(A)}) + \mathsf{H}^{2}(\mu_{0}^{B}, \mu_{1}^{T(B)}).
$$

Moreover, using the decompositions

$$
\mu_0 = (\mu_0 - \mu_0^A - \mu_0^B) + \mu_0^A + t\mu_0^B + (1-t)\mu_0^B \text{ and}
$$
  
$$
\mu_1^{t,s} = (\mu_1 - \mu_1^{T(A)} - \mu_1^{T(B)}) + (1-s)\mu_1^{T(A)} + t\mu_0^B + \mu_1^{T(B)},
$$

we can apply the subadditivity for HK<sup>2</sup> (cf. [LMS18, Lem. 7.8]) and obtain

$$
\begin{split} \mathsf{H}\mathsf{K}^2(\mu_0,\mu_1^{t,s}) &\leq \mathsf{H}\mathsf{K}^2(\mu_0 - \mu_0^A - \mu_0^B, \mu_1 - \mu_1^{T(A)} - \mu_1^{T(B)}) \\ &+ \mathsf{H}\mathsf{K}^2(\mu_0^A, (1-s)\mu_1^{T(A)}) + 0 \ + \mathsf{H}\mathsf{K}^2((1-t)\mu_0^B, \mu_1^{T(B)}). \end{split}
$$

Combining this with the above splitting identity we find

$$
HK^{2}(\mu_{0}, \mu_{1}) \geq AK^{2}(\mu_{0} - \mu_{0}^{A} - \mu_{0}^{B}, \mu_{1} - \mu_{1}^{T(A)} - \mu_{1}^{T(B)}) + K^{2}(\mu_{0}^{A}, \mu_{1}^{T(A)}) - K^{2}(\mu_{0}^{A}, (1-s)\mu_{1}^{T(A)}) + K^{2}(\mu_{0}^{B}, \mu_{1}^{T(B)}) - K^{2}((1-t)\mu_{0}^{B}, \mu_{1}^{T(B)}).
$$

The terms in the second and third line can be further estimated by the conditions S.1 and S.2, namely

$$
\mu_1^{\mathbf{T}(A)}(X) = \mu_1(\mathbf{T}(A)) \ge (1+\epsilon)\mu_0(A) = (1+\epsilon)\mu_0^A(X) \text{ and}
$$
  
$$
\mu_1^{\mathbf{T}(B)}(X) = \mu_1(\mathbf{T}(B)) \le (1-\epsilon)\mu_0(B) = (1-\epsilon)\mu_0^B(X),
$$

and the scaling of HK<sup>2</sup> in Proposition 3.9. For s ∈ [0, s<sup>ǫ</sup> ] with s<sup>ǫ</sup> := 1 − 1/(1+ǫ) <sup>2</sup> we find

$$
\begin{aligned} \mathsf{H}\mathsf{K}^{2}(\mu_{0}^{A}, (1-s)\mu_{1}^{\mathbf{T}(A)}) &= \sqrt{1-s} \, \mathsf{H}\mathsf{K}^{2}(\mu_{0}^{A}, \mu_{1}^{\mathbf{T}(A)}) - \left(1 - \sqrt{1-s}\right) \left(\sqrt{1-s} \, \mu_{1}(\mathbf{T}(A)) - \mu_{0}(A)\right) \\ &\leq \mathsf{H}\mathsf{K}^{2}(\mu_{0}^{A}, \mu_{1}^{\mathbf{T}(A)}). \end{aligned}
$$

Similarly, for t ∈ [0, t<sup>ǫ</sup> ] with t<sup>ǫ</sup> = 1 − (1−ǫ) <sup>2</sup> = ǫ(2−ǫ), we have

$$
\begin{aligned} \mathsf{H}\mathsf{K}^2((1-t)\mu_0^B, \mu_1^{\mathbf{T}(B)}) &= \sqrt{1-t} \, \mathsf{H}\mathsf{K}^2(\mu_0^B, \mu_1^{\mathbf{T}(B)}) - \left(1 - \sqrt{1-t}\right) \left(\sqrt{1-t} \, \mu_0(B) - \mu_1(\mathbf{T}(B))\right) \\ &\leq \mathsf{H}\mathsf{K}^2(\mu_0^B, \mu_1^{\mathbf{T}(B)}). \end{aligned}
$$

We can now choose t<sup>∗</sup> ∈ ]0, t<sup>ǫ</sup> [ and s<sup>∗</sup> ∈ ]0, s<sup>ǫ</sup> [ with s∗µ T(A) 1 (X) = t∗µ B 0 (X) such that µ <sup>t</sup>∗,s<sup>∗</sup> (X) = 1. By the above construction we have HK<sup>2</sup> (µ0, µ1) <sup>≥</sup> HK<sup>2</sup> (µ0, µ t∗,s∗ 1 ) which implies on (P(X), SHK) the desired estimate

$$
\mathsf{SK}^{2}(\mu_0, \mu_1) \geq \mathsf{SK}^{2}(\mu_0, \mu_1^{t_*, s_*}).
$$

To see that E(µ t,s 1 ) < E(µ1), we repeat the arguments in Steps 3 and 4 of the proofs of Propositions 4.2 and 4.3.

### 4.5 Many minimization steps

Applying the MM scheme means to apply the minimization problems (4.2) iteratively. If we repeat the minimization problems, we will show that the density bounds are such that we keep good a priori bounds that depend only on the actual time t = nτ but not on the number of steps.

For the MM scheme of (M(X), E,HK) we have derived the upper and lower bounds for ρ<sup>1</sup> that depend on

$$
\underline{\rho}_0 := \mathrm{ess\,inf}\ \rho_0 \quad \text{and} \quad \overline{\rho}_0 = \mathrm{ess\,sup}\ \rho_0
$$

in the form

$$
\mathsf{H} \colon \min\left\{b, \frac{\underline{\rho}_0}{(1+2\tau \max\{E'(b), 0\})^2}\right\} \le \rho_1(x) \le \max\left\{a, \frac{\overline{\rho}_0}{(1+2\tau \min\{E'(a), 0\})^2}\right\},\tag{4.25}
$$

$$
\mathbf{SK}: \qquad \qquad \underline{\rho}_0 \le \rho_1(x) \le \overline{\rho}_0,\tag{4.26}
$$

where a, b > 0 are arbitrary as long as 2τE′ (a) > −1.

Thus, when constructing µ τ <sup>n</sup> by the MM scheme (4.2) we easily can apply these bounds and obtain µ τ <sup>n</sup> = ρnν with the corresponding density bounds. If E ′ has a positive zero c<sup>∗</sup> > 0, then changes sign, we immediately obtain a global bound for all iterates in the form

$$
\min\{c_*, \underline{\rho}_0\} \le \rho_k(x) \le \max\{c_*, \overline{\rho}_0\} \quad \text{a.e. in } X.
$$

Thus, the more difficult cases are when E is either strictly decreasing (i.e. E′ (c) < 0 for all c > 0) of strictly increasing (i.e. E′ (c) > 0 for all c > 0). Both these cases can occur and are relevant, e.g. for the choices E(c) = − √ c or E(c) = c 2 .

Also in these general cases we are able to provide suitable upper and lower density bounds that only depend on kτ , which is the original time in the gradient-flow equation. Proposition 4.5 (General lower and upper density bounds for HK) *Assume that* <sup>E</sup> : [0, <sup>∞</sup>) <sup>→</sup> **<sup>R</sup>** *is lsc, convex. Assume* <sup>0</sup> < ρ<sup>0</sup> ≤ ρ(x) ≤ ρ<sup>0</sup> < ∞ *and set and set*

$$
\underline{S} := \sup \{ E'(c) \mid c \in (0, \underline{\rho}_0] \} \le \overline{S} := \inf \{ E'(c) \mid c \ge \overline{\rho}_0 \} > -\infty.
$$

*Assume further* τS ≥ −1/4*, then for all* <sup>k</sup> <sup>∈</sup> **<sup>N</sup>** *we have* <sup>µ</sup> τ <sup>k</sup> = ρ τ k ν *where* ρ τ k *satisfies, for all* <sup>k</sup> <sup>∈</sup> **<sup>N</sup>***, the general density bounds*

$$
\underline{\rho}_0 e^{-4 \max\{\underline{S}, 0\} k \tau} \le \rho_k^{\tau}(x) \le \overline{\rho}_0 e^{8 \max\{-\overline{S}, 0\} k \tau} \ a.e. \ in \ X. \tag{4.27}
$$

Proof. The result follows essentially by iterating (4.25).

Step 1: Upper bound. We construct a nondecreasing sequence (ak) via a<sup>0</sup> = ρ<sup>0</sup> and the recursion ak+1 = ak/(1+2τ min{E′ (ak), 0}) <sup>2</sup> ≥ ak. Using the upper estimate in (4.25) an an induction over k, we obtain for ρ<sup>k</sup> := ess supρ τ k , the estimate

$$
\rho_k^{\tau}(x) \le \overline{\rho}_k \le a_k \quad \text{for all } k \in \mathbb{N}.
$$

However, the monotonicities of (ak) and of E′ imply min{E′ (ak), 0} ≥ min{E′ (ak), 0} = min{S, 0} =. Thus, the recursion for a<sup>k</sup> implies

$$
a_k \le \frac{a_0}{\left(1+2\tau\min\{\overline{S},0\}\right)^{2k}} \stackrel{*}{\le} a_0 \left(1-4\tau\min\{\overline{S},0\}\right)^{2k} \le \overline{\rho}_0 e^{8\max\{-\overline{S},0\}k\tau},
$$

where in ∗ ≤ we used τS ≥ −1/4 and the estimate (1−r) <sup>−</sup><sup>2</sup> ≤ 1+2r for all r ∈ [0, 1/2].

Step 2: Lower bound. We proceed analogously and define (bk)k∈**<sup>N</sup>** via b<sup>0</sup> = ρ 0 and the recursion bk+1 = bk/(1+2τ max{E′ (bk), 0}) <sup>2</sup> ≤ bk. This yields

$$
\rho_k^{\tau}(x) \ge \underline{\rho}_k \ge b_k \ge \frac{\underline{\rho}_0}{\left(1+2\tau \max\{\overline{S},0\}\right)^{2k}} \ge \underline{\rho}_0 e^{-4\max\{\underline{S},0\}k\tau},
$$

which is the desired result.

The upper bound can be improved from exponential in t = kτ into quadratic, if we impose a suitable lower bound for E′ , namely E′ (c) ≥ −e∗/ √ c for c ≥ c<sup>∗</sup> and some e<sup>∗</sup> ≥ 0.

Proposition 4.6 (Iteration of the upper bound for HK) *Assume that* <sup>E</sup> : [0, <sup>∞</sup>) <sup>→</sup> **R** *is lsc, convex, and satisfies* E ′ (c) ≥ −e∗/ √ c *for* c ≥ c∗*. Then,* ρ<sup>k</sup> = ess sup ρ τ k *satisfies*

$$
\overline{\rho}_k \le \left(\sqrt{\max\{\overline{\rho}_0, c_*, 4\tau^2 e_*^2\}} + 4e_* k\tau\right)^2 \text{ for all } k \in \mathbb{N},\tag{4.28}
$$

Proof. Iteration of the upper bound: We apply the upper bound for HK iteratively using ρ<sup>k</sup> = ess sup ρ<sup>k</sup> with ρ<sup>0</sup> = cmax by choosing suitable a = ak. Clearly, the estimate is monotone in ρ<sup>k</sup> , hence we may replace ρ<sup>0</sup> by max{ρ<sup>0</sup> , c∗, 4τ 2 e 2 ∗ , cmax}. Thus, we obtain ρ<sup>k</sup> ≥ ρ<sup>0</sup> ≥ c<sup>∗</sup> and choosing a<sup>k</sup> = ρ<sup>k</sup> is admissible because 2τE′ (ak) ≥ −2τe∗/ √ ρ<sup>k</sup> ≥ −1/2 > −1. Moreover, we obtain

$$
\sqrt{\overline{\rho}_{k+1}} \leq \frac{\sqrt{\overline{\rho}_k}}{1 + 2\tau e_*/\sqrt{\overline{\rho}_k}} \leq \sqrt{\overline{\rho}_k} \left(1 + 4\tau e_*/\sqrt{\overline{\rho}_k}\right) = \sqrt{\overline{\rho}_k} + 4\tau e_*.
$$

where we used the estimate (1−α) <sup>−</sup><sup>1</sup> ≤ 1+2α for α ∈ [0, 1/2]. Thus, the desired estimate (4.28) follows.

# 5 Existence for EVI using local κ-concavity

Following [Sav07, LaM19] we first give precise definitions of geodesics curves in a general metric space (X, d) and the local-angle condition (LAC). Based on these fundamental concepts, [Sav11, MuS23] introduces the two geometry-descriptive functions h·, ·iup and **<sup>∆</sup>**(·, ·) (cf. Definition 5.5) that allow us to quantify the relationship between two geodesics emanating from the same point. Next we discuss semiconvex and semiconcave functions in the sense of geodesic κ-concavity or geodesic λ-convexity. When the squared distance <sup>1</sup> 2 d 2 X (t, xob) is κ-concave along a geodesic xy with respect to some observer xob the derivative of <sup>t</sup> 7→ <sup>1</sup> 2 d 2 X ( xy (t), xob) can be estimated in terms of the two quantities mentioned above. To illustrate the concepts of the LAC and the semiconcavity of the squared distance, we consider a few simple examples in Section 5.4.

Finally, we discuss Evolutionary Variational Inequalities EVI<sup>λ</sup> for a metric gradient system (X, φ, dX) and show that solutions can be constructed via the minimizing movement scheme, if φ is strongly λ-convex with compact sublevels and the closure of dom(φ) can be written as the closure of the union of sets Aκ, where <sup>1</sup> 2 d 2 X is κ-concave. See Theorem 5.9 for the exact statement.

The main arguments for our existence theory are based on results developed in [Sav07, Sav11] which was a prelude for the work in [MuS20] and [MuS23]. Hence, we provide the corresponding result here without our own proofs expecting a soon publication of the latter work. This contains in particular Proposition 5.7 and the estimates on the Minimizing Movement scheme in Section 5.5. We would like however to remark that we extend some of the results in [MuS23] by weakening the assumption that the squared distance must be universally K-concave for some K > 0. Instead, we assume that we have concave bounds K<sup>n</sup> for a collection of nested sets An, whose closure is the domain of φ. Proofs that depend on this modified assumption are provided in both versions of the paper.

### 5.1 Geodesic spaces and the local angle condition (LAC)

We now provide some basic definitions for geodesics in metric spaces and some of their properties.

Definition 5.1 (Geodesics) *Let* (X, <sup>d</sup>X) *be a metric space. A curve* xy : [0, 1] <sup>→</sup> <sup>X</sup> *is called a* (constant-speed) geodesic joining x to y *if*

$$
\widehat{xy}(0) = x, \quad \widehat{xy}(1) = y, \quad and
$$
  
$$
\mathsf{d}_{\mathfrak{X}}(\widehat{xy}(t_1), \widehat{xy}(t_2)) = |t_2 - t_1| \mathsf{d}_{\mathfrak{X}}(x, y) \text{ for all } t_1, t_2 \in [0, 1].
$$

*We will denote the set of all such geodesics with* Geod(x, y)*.*

*The metric space* (X, <sup>d</sup>X) *is called a* geodesic space*, if for all points* x, y <sup>∈</sup> <sup>X</sup> *the set* Geod(x, y) *is nonempty. The metric* d<sup>X</sup> *is then called a* geodesic distance*.*

For our theory it will be important to introduce the concept of geodesic covers of an arbitrary set. It plays the role of the convexification in Banach spaces, however, our covering notion is not idempotent, i.e. AGeod \$ AGeodGeod is possible.

Definition 5.2 (Geodesic cover) *Let* (X, dX) *be a metric space and* A *a subset of* X*. We define the* geodesic cover AGeod *of* A *by*

$$
A^{\text{Geod}} := \left\{ \widehat{xy}(t) \mid t \in [0,1], \widehat{xy} \in \text{Geod}(x,y), \ x, y \in A \right\}.
$$
 (5.1)

In the following we introduce the two geometric concepts in geodesic metric spaces called the *local angle condition* (LAC) and κ*-concavity*. These properties are going to be utilized in the sequel to prove that the curves occurring by geodesically interpolating the points produced by the minimizing scheme, converge to solutions of the EVIλ, when the minimization step τ tends to zero. In order to introduce LAC, we first introduce the notions of comparison angle between three points and of local angle between two geodesics emanating from the same point.

Definition 5.3 (Comparison and local angles) *Let* (X, dX) *be a metric space. For three points* x, y, z ∈ X *with* x 6∈ {y, z}*, we set*

$$
A(x; y, z) := \frac{d_{\mathfrak{X}}^2(x, y) + d_{\mathfrak{X}}^2(x, z) - d_{\mathfrak{X}}^2(y, z)}{2d_{\mathfrak{X}}(x, y)d_{\mathfrak{X}}(x, z)} \in [-1, 1]
$$

*and define the* comparison angle (x; y, z) <sup>∈</sup> [0, π] *with vertex* <sup>x</sup> *via*

$$
\sphericalangle(x; y, z) := \arccos(A(x; y, z)) \in [0, \pi].
$$

*Let* xy *and* xz *be two geodesics in* X *emanating from point* x = xy (0) = xz (0). *The* upper angle <sup>∢</sup>up( xy , xz ) <sup>∈</sup> [0, π] *and the* lower angle <sup>∢</sup>lo( xy , xz ) <sup>∈</sup> [0, π], *between* xy *and* xz *are defined by*

$$
\langle \mathcal{A}_{\text{up}}(\widehat{x}\widehat{y}, \widehat{x}\widehat{z}) := \limsup_{s,t\downarrow 0} \langle x, \widehat{x}\widehat{y}(s), \widehat{x}\widehat{z}(t) \rangle,
$$
  
$$
\langle \mathcal{A}_{\text{lo}}(\widehat{x}\widehat{y}, \widehat{x}\widehat{z}) := \liminf_{s,t\downarrow 0} \langle x, \widehat{x}\widehat{y}(s), \widehat{x}\widehat{z}(t) \rangle.
$$
 (5.2)

*If* ∢up(xy , xz ) = ∢lo(xy , xz ) *holds, we say that the* local angle *exists in the strict sense and write* ∢(xy , xz )*.*

The local angle condition concerns three geodesics emanating from one point and states that the sum of the three local angles does not exceed 2π.

Definition 5.4 (Local angle condition) *We say that a point* x *of a geodesic metric space* (X, d) *satisfies* LAC*, if for any three geodesics* xy , xz , xw *emanating from* x, *we have*

$$
\langle \zeta_{\text{up}}(\widehat{x}\widehat{y},\widehat{x}\widehat{z}) + \langle \zeta_{\text{up}}(\widehat{x}\widehat{z},\widehat{x}\widehat{w}) + \langle \zeta_{\text{up}}(\widehat{x}\widehat{w},\widehat{x}\widehat{y}) \rangle \leq 2\pi.
$$

In geodesic spaces (X, d) the set of all geodesics emanating from a point x may be considered as a (nonlinear) surrogate of the tangent space defined in the case of manifolds. We now introduce a kind of scalar product between two geodesics emanating from one point as a generalization of the classical inner product on the tangent space. Moreover, we define the function **∆** that measures how much two geodesic curves emanating from one point are exactly opposite to each other.

Definition 5.5 (Comparing two geodesics) *In the geodesic space* (X, dX) *consider three points* x, y, z ∈ X *and two geodesics* xy ∈ Geod(x, y), xz ∈ Geod(x, z) *and define*

$$
\langle \widehat{xy}, \widehat{xz} \rangle_{\text{up}} := \mathsf{d}_{\mathfrak{X}}(x, y) \mathsf{d}_{\mathfrak{X}}(x, z) \cos(\langle \mathfrak{t}_{\text{up}}(\widehat{xy}, \widehat{xz}) \rangle)
$$
  
= 
$$
\liminf_{s, t \downarrow 0} \frac{1}{2st} \left( \mathsf{d}_{\mathfrak{X}}^2(x, \widehat{xy}(s)) + \mathsf{d}_{\mathfrak{X}}^2(x, \widehat{xz}(t)) - \mathsf{d}_{\mathfrak{X}}^2(\widehat{xy}(s), \widehat{xz}(t)) \right)
$$
(5.3)

$$
\Delta^{2}(\widehat{x}\widehat{y},\widehat{x}\widehat{z}) = d_{\mathfrak{X}}^{2}(x,y) + d_{\mathfrak{X}}^{2}(x,z) + 2\langle \widehat{x}\widehat{y},\widehat{x}\rangle_{\text{up}} \geq 0.
$$
\n(5.4)

The second form of hxy , xz iup given in (5.3) is easily derived from the above definition when taking into account that cos is decreasing on [0, π], which turns the limsup in ∢up into a liminf.

Considering a Hilbert space with scalar product (x|y) and norm kzk the geodesic curves are given via xy = (1−t)x + ty, xz = (1−t)x + tz and we easily find

$$
\langle \widehat{xy}, \widehat{xz} \rangle_{\text{up}} = (y-x|z-x) \quad \text{and} \quad \mathbb{\Delta}^2(\widehat{xy}, \widehat{xz}) = ||(y-x)-(z-x)||^2,
$$

i.e. we have **∆** 2 ( xy , xz ) = 0 if and only if x = 1 2 (y+z). For general geodesic spaces we may consider a geodesic yz ∈ Geod(y, z), choose x as the midpoint yz (1/2), and set

xy (t) = yz ((1−t)/2) and xz (t) = yz ((1+t)/2) for t ∈ [0, 1].

Then, xy <sup>∈</sup> Geod(x, y), xz <sup>∈</sup> Geod(x, z) and **<sup>∆</sup>** 2 (xy , xz ) = 0.

### 5.2 Semiconvex and semiconcave functions

For a geodesic metric space (X, dX), we now provide the definition of κ-(semi)concavity and <sup>λ</sup>-(semi)convexity of a function <sup>φ</sup> : <sup>X</sup> <sup>→</sup> (−∞, <sup>∞</sup>] along a geodesic (X, <sup>d</sup>X) or of a functional F : X × X → (−∞, ∞], along some geodesic with respect to some observer. For this, we recall that a function f : [0, 1] → (−∞, ∞] is called κ-concave or λ-convex, if the mapping t 7→ f−κt<sup>2</sup> /2 is concave or t 7→ f−λt<sup>2</sup> /2 is convex, respectively. We emphasize that κ and λ can lie in all of **R**. Subsequently, we will shortly say κ-concave and λ-convex.

For a κ-concave function f : [0, 1] → (−∞, ∞] we have the inequality

$$
f \kappa
$$
-concave  $\implies \forall t \in [0, 1]:$   $f(t) + \frac{1}{2} \kappa t (1-t) \ge (1-t) f(0) + t f(1).$  (5.5)

For the following definition we recall that the elements of Geod(x, y) are constantspeed geodesics of length and speed dX(x, y).

Definition 5.6 (κ-concavity/convexity) *Let* (X, dX) *be a geodesic space,* A *and* B *subsets of* <sup>X</sup>*, and* <sup>κ</sup> <sup>∈</sup> **<sup>R</sup>***.*

*(A) A function* φ : X → (−∞, ∞] *is called* κ-concave (convex) on A*, if for all* x, y ∈ A *there exists an* xy <sup>∈</sup> Geod(x, y) *such that the function* <sup>t</sup> 7→ <sup>φ</sup>(xy (t)) *is* <sup>κ</sup> <sup>d</sup> 2 X (x, y)*-concave (convex).* φ *is called* strongly κ-concave (convex) on A*, if the previous condition holds for all* xy ∈ Geod(x, y)*. If* A = X*, then we simply that* φ *is (strongly)* κ*-concave (convex).*

*(B) For* x, y ∈ X, *we say that a functional* F : X × X → (−∞, ∞] *is* κ*-concave (convex) along a geodesic* xy ∈ Geod(x, y) *with respect to the observer* xob ∈ X*, if* <sup>t</sup> 7→ <sup>F</sup>( xy (t), xob) *is* <sup>κ</sup> <sup>d</sup> 2 X (x, y)*-concave (convex). Furthermore, we say that* F *is* κ*concave (convex) in* A *with respect to observers from* B*, if for every couple of points* x, y ∈ A, *there exists a geodesic* xy ∈ Geod(x, y) *such that for every* xob ∈ B *we have that* F *is* κ*-concave (convex) along* xy *with respect to* xob*. We finally say that* F *is* strongly κ*-concave (convex) in* A *with respect to observers from* B*, if for every couple of points* x, y ∈ A*, all geodesics* xy ∈ Geod(x, y)*, and all* xob ∈ B *the function* F *is* κ*-concave (convex) along* xy *with respect to* xob*.*

In the previous definition, the points xy (t) don't have to lie in A for t ∈ (0, 1). Of course, we have xy (t) ∈ AGeod for all t ∈ [0, 1].

A crucial step in the convergence theory of minimizing movement solutions is to exploit the κ-concavity of the squared distance with respect to suitable observers xob. The point is that one obtains an upper estimate of the upper right Dini derivative <sup>d</sup><sup>+</sup> dt 1 2 d 2 X (xy (t), xob) in terms of the two geometric quantities h·, ·iup and **<sup>∆</sup>** introduced in Definition 5.5. Here the upper right Dini derivative is defined via

$$
\frac{\mathrm{d}^+}{\mathrm{d}t}\zeta(t) := \limsup_{h \downarrow 0} \frac{1}{h}(\zeta(t+h) - \zeta(t)).
$$

For the proof of the following result we again refer to [Sav11, MuS23].

Proposition 5.7 (Differentiation of squared distance along geodesics) *If* (X, dX) *is a geodesic space and* x, y ∈ X *such that* F = 1 2 d 2 X *is* κ*-concave along* xy ∈ Geod(x, y) *with respect to an observer* xob*, then we have*

$$
\frac{\mathrm{d}^+}{\mathrm{d}t} \frac{1}{2} \mathsf{d}^2_{\mathfrak{X}}(\widehat{x}\mathfrak{Y}(t), x_{\text{ob}}) \le -\langle \widehat{x}\mathfrak{Y}, \widehat{x x_{\text{ob}}} \rangle_{\text{up}} + \kappa \mathsf{d}_{\mathfrak{X}}(x, y) \mathsf{d}_{\mathfrak{X}}(\widehat{x}\mathfrak{Y}(t), x) \ a.e. \ on \ [0, 1]. \tag{5.6}
$$

*If furthermore* (X, <sup>d</sup>X) *satisfies LAC at* x, *then for a.a.* <sup>t</sup> <sup>∈</sup> [0, 1] *we have*

$$
\frac{d^+}{dt}\frac{1}{2}d^2_{\mathcal{X}}(\widehat{x}\mathfrak{F}(t),x_{\text{ob}}) \le \langle \widehat{xz}, \widehat{xx_{\text{ob}}}\rangle_{\text{up}} + d_{\mathcal{X}}(x,x_{\text{ob}})\mathbb{A}(\widehat{x}\mathfrak{F},\widehat{x}\mathfrak{F}) + t\kappa d^2_{\mathcal{X}}(x,y),\tag{5.7}
$$

*for every* z ∈ X *and* xz ∈ Geod(x, z).

For an illustration of the estimates in Proposition 5.7 we refer to the simple case discussed in Example 5.12.

## 5.3 EVI<sup>λ</sup> and construction of solutions

We first recall the standard definitions and results from [MuS20, Sec. 3] and then introduce our notations.

We have the following theorem that provides an alternative form of (1.2) that uses integration instead of differentiation. This form can, in a straightforward manner, be combined with the lower semicontinuity properties of the distance d<sup>X</sup> and of φ, thus allowing to show that various limits of EVI<sup>λ</sup> solutions are again EVI<sup>λ</sup> solutions.

The two results given in the next theorem are taken from [MuS20, Thm. 3.3+3.5].

#### Proposition 5.8 (Characterizations and properties of EVI solutions)

*(A) A curve* x : [0, T) → X *satisfies EVI*<sup>λ</sup> *with respect to* φ*, if and only if for all* <sup>x</sup>ob <sup>∈</sup> dom(φ) *the two maps* <sup>t</sup> 7→ <sup>φ</sup>(x(t)) *and* <sup>t</sup> 7→ <sup>d</sup> 2 (x(t), xob) *belong to* L 1 loc((0, T)) *and*

$$
\frac{1}{2}\mathsf{d}_{\mathcal{X}}^{2}(\boldsymbol{x}(t),x_{\mathsf{ob}})-\frac{1}{2}\mathsf{d}_{\mathcal{X}}^{2}(\boldsymbol{x}(s),x_{\mathsf{ob}})+\int_{s}^{t}\Big(\phi(\boldsymbol{x}(r))+\frac{\lambda}{2}\mathsf{d}_{\mathcal{X}}^{2}(\boldsymbol{x}(r),x_{\mathsf{ob}})\Big)\,\mathrm{d}r\leq(t-s)\phi(x_{\mathsf{ob}})
$$
\n
$$
\text{for all }s,t\in(0,T) \text{ with } s
$$

*(B) If* x : [0, T) → X *is an EVI*<sup>λ</sup> *solution, then* t 7→ φ(x(t)) *is non-increasing and hence continuous from the right (by lsc of* φ*).*

*(C) If* x 1 , x 2 : [0, ∞) → X *are EVI*<sup>λ</sup> *solutions, then we have*

$$
\mathsf{d}_{\mathfrak{X}}(\boldsymbol{x}^1(t),\boldsymbol{x}^2(t)) \le \mathrm{e}^{-\lambda(t-s)}\mathsf{d}_{\mathfrak{X}}(\boldsymbol{x}^1(s),\boldsymbol{x}^2(s)) \quad \text{for all } s,t \in [0,T) \text{ with } s < t. \tag{5.9}
$$

We are now able to formulate our main existence result for EVI<sup>λ</sup> solutions for metric gradients systems (X, dX, φ) which relies on the geodesic structure of (X, dX), the λconvexity of the potential φ, and some "local κ-concavity" of <sup>1</sup> 2 d 2 X (t, xob). The construction of solutions will be done by the minimizing movement scheme and geodesic interpolation. For a given x ∈ X and a time step τ the discrete solutions (x τ n )n=0,1,...,N of the minimization movement schemes are defined via

$$
x_0^{\tau} = x \quad \text{and} \quad x_n^{\tau} \in \arg\min\left\{ \left. \frac{1}{2\tau} \mathsf{d}_{\mathfrak{X}}^2(x, x_{n-1}^{\tau}) + \phi(x) \; \right| \; x \in \mathfrak{X} \right\} \text{ for } n \in \mathbb{N}. \tag{5.10}
$$

As all our <sup>φ</sup> are <sup>λ</sup>-convex for some <sup>λ</sup> <sup>∈</sup> **<sup>R</sup>**, the functional <sup>x</sup> 7→ <sup>1</sup> 2τ d 2 X (x, x<sup>τ</sup> n−1 ) + φ(x) is quadratically bounded from below for all τ > 0 with λ + 1/τ > 0. Thus under suitable assumptions on φ minimizers exist for sufficiently small τ .

Theorem 5.9 (Existence of EVI<sup>λ</sup> solutions) *Let* (X, dX) *be a geodesic metric space and* φ : X → (−∞, ∞] *a strongly* λ*-convex functional with compact sublevels. Further assume that there exists a nested sequence of sets* A<sup>κ</sup> ⊂ dom(|∂φ|) *with* ∪A<sup>κ</sup> = dom(φ), *for which the statements (A1) to (A3) are true:*

- *(A1) For every* <sup>κ</sup> <sup>∈</sup> **<sup>N</sup>**, *we have that* <sup>1</sup> 2 d 2 X : <sup>X</sup> <sup>×</sup> <sup>X</sup> <sup>→</sup> **<sup>R</sup>** <sup>+</sup> *is strongly* κ*-concave in* A<sup>κ</sup> *with observers in* AGeod κ , *and satisfies LAC for every point in* Aκ.
- *(A2) For all* x<sup>0</sup> ∈ ∪κ∈**<sup>N</sup>**A<sup>κ</sup> *there exists* κ<sup>0</sup> *such that for all* κ > κ<sup>0</sup> *there exists* T(x0, κ) > 0 *such that for all time steps* τ > 0 *the* n*-step minimization scheme* (x τ k )k=0,...,n *remains in* A<sup>κ</sup> *as long as* n < T/τ + 1*.*
- *(A3) For every* x ∈ dom(φ), *there exists a sequence* x<sup>m</sup> ∈ ∪A<sup>κ</sup> *with* x<sup>m</sup> → x *and* φ(xm) → φ(x).

*Then, for every* x<sup>0</sup> ∈ ∪A<sup>κ</sup> *there exists a unique EVI*<sup>λ</sup> *solution* x : [0, T∞(x0)) → X *with* x(0) = x0*, where* T∞(x0) = limκ→∞ T(x0, κ) *(w.l.o.g.* κ 7→ T(x0, κ) *is non-decreasing).*

*If additionally, assumption (A2) holds for all* x<sup>0</sup> ∈ ∪A<sup>κ</sup> *with* T∞(x0) = ∞*, then for all* <sup>x</sup>b<sup>0</sup> <sup>∈</sup> dom(φ) *there exists a complete EVI*<sup>λ</sup> *solution* <sup>x</sup> *with* <sup>x</sup>(0) = <sup>x</sup>b<sup>0</sup>*.*

We note that our assumption x<sup>0</sup> ∈ ∪A<sup>κ</sup> trivially implies x<sup>0</sup> ∈ dom(∂φ), i.e. |∂φ|(x0) < ∞. For λ-convex functionals on always has dom(∂φ) = dom(φ), hence general initial conditions in dom(φ) can be approximated.

The proof will be completed in Section 5.6 after the necessary a priori estimates for the discrete minimizing movement solutions are collected next.

### 5.4 Examples and counterexamples for LAC and semiconcavity

To help the intuition about the two important geometric conditions for (X, d) we provide some examples and counterexamples for LAC and for semiconcavity of <sup>1</sup> 2 <sup>d</sup>(xob, ·) 2 .

Clearly, it can be easily verified that a Hilbert space satisfies the LAC and the squared norm is κ-concavity with κ = 1 (as well as λ-convexity with λ = 1). The properties are still valid on convex subsets of a Hilbert space. More generally, on any smooth Riemannian manifold LAC is satisfied and κ-concavity holds on compact sets.

Example 5.10 (LAC and semiconcavity not satisfied) We consider the "cross"

$$
M = \{ x = (x_1, x_2) \in \mathbb{R} \mid x_1 = x_2 = 0 \} = (\mathbb{R} \times \{0\}) \cup (\{0\} \times \mathbb{R})
$$

together with the distance

$$
\mathsf{d}(x,y) = |x_1-y_1| + |x_2-y_2| = ||x-y||_1,
$$

which is also the induced metric obtained by restricting the Euclidean distance in **R** 2 .

We may consider the following three geodesics starting from x<sup>∗</sup> = 0:

$$
\gamma^{(1)}(s) = (s, 0), \quad \gamma^{(2)}(s) = (-s, 0), \quad \gamma^{(3)}(s) = (0, -s).
$$

Since the (unique) geodesic connecting two points from two different geodesics always has to pass through x<sup>∗</sup> = 0 we easily see that all three angles ∢(γ (i) , γ(j) ) for i 6= j are equal to π, which violates LAC.

Moreover, the unique arclength-parametrized geodesic connecting y<sup>∗</sup> = (−1, 0) and z<sup>∗</sup> = (0, 1) is given by

$$
\widehat{y_*z_*}(s) = \gamma^{(1)}(1-s) \text{ for } s \in [0,1] \quad \text{and} \quad \widehat{y_*z_*}(s) = \gamma^{(3)}(s-1) \text{ for } s \in [1,2]. \tag{5.11}
$$

Hence, choosing the observer <sup>x</sup>ob = (1, 0) we obtain <sup>d</sup>(xob, <sup>y</sup>∗z<sup>∗</sup> (s))<sup>2</sup> = max{(2−s) 2 , s<sup>2</sup>}, which is not semiconcave.

The next example shows that LAC and semiconcavity are not to be expected in general Banach spaces, unless the squared norm is semiconcave.

Example 5.11 (LAC and semiconcavity in Banach spaces) LAC is satisfied in a Banach space if and only if it is a Hilbert space, see [Sav07, p. 153].

We consider the Banach space **R** <sup>2</sup> with the norm kxk<sup>p</sup> = |x1| <sup>p</sup>+|x2| p <sup>1</sup>/p .

For p = 1 we may consider the same geodesics γ (i) as in the previous example and obtain ∢(γ (i) , γ(j) ) = π for i 6= j are equal to π, which violates LAC. For the points xob, y∗, z<sup>∗</sup> as in the previous example y∗z<sup>∗</sup> given in (5.11) is still one of the many geodesics in Geod(y∗, z∗), and we have kxob−y∗z<sup>∗</sup> (s)k 2 <sup>1</sup> = min{(2−s) 2 , 1 + (s−1)<sup>2</sup>} which is not semiconcave.

The case p = ∞ is analogous to the case p = 1. For p ∈ ]1, 2[ it can be shown that local angles are not defined in the strict sense and that semiconcavity holds. The case p = 2 is the Hilbertian case with LAC and 1-concavity.

For <sup>p</sup> <sup>∈</sup> ]2, <sup>∞</sup>[ one still has semiconcavity because sn(x) = <sup>1</sup> 2 kxk 2 p is lies in C 1 (**R** 2 ) with Lip(∇sn) = c<sup>p</sup> ≤ p, i.e. we have p-concavity.

For general Banach spaces (B, k· kB), one obtains semiconcavity if the sn(x) = <sup>1</sup> 2 kxk 2 B is differentiable with a globally Lipschitz-continuous derivative.

The next example shows a case where semiconcavity and LAC are present, but vanish if certain parameter of the space are taken to a nontrivial limit.

Example 5.12 (A smoothed three-quarters disc) For 0 ≤ r < R ≤ ∞, we consider the nonconvex two-dimensional domain

$$
X_{r,R} := \left\{ x \in \mathbb{R}^2 \mid |x| \le R \text{ and } (x_1 \ge 0 \text{ or } x_2 \ge 0) \right\} \cup \left\{ x \in [-r,0]^2 \mid |x+(r,r)| \ge r \right\},\
$$

where |·| denotes the Euclidean distance, see also Figure 5.4. The distance <sup>d</sup> is given by the length of the shorted curve inside of Xr,R. It is easy to see that all geodesics are unique and are either straight lines or they touch the circle of radius r around (−r, −r). Obviously, the geodesics connecting x<sup>∗</sup> = (−R, 0) and y<sup>∗</sup> = (R, 0) is the straight line γ (1)(s) = (R+s, 0) for s ∈ [0, 2R], and the geodesics γ (2) = x∗z<sup>∗</sup> connecting x<sup>∗</sup> and z<sup>∗</sup> = (0, −R) coincides with γ (1) for s ∈ [0, R−r], then branches of tangentially to stay on the circle for <sup>s</sup> <sup>∈</sup> [R−r, R+( <sup>π</sup> <sup>2</sup>−1)r] and then moves down vertically for <sup>s</sup> <sup>∈</sup> [R+( <sup>π</sup> <sup>2</sup>−1)r, <sup>2</sup>R+( <sup>π</sup> <sup>2</sup>−2)r].

For r > 0 the LAC is always satisfied, and it can be shown that the <sup>1</sup> 2 d 2 (xob, ·) is κ-concave with κ = √ 2 + R/r. The most extreme case is achieved by observing γ (2) from <sup>x</sup>ob = (R/<sup>√</sup> 2, R/<sup>√</sup> 2).

In the limit r = 0 the LAC is lost, which follows as in Example 5.10. For r > 0 and R = ∞ we still have the LAC, while semiconcavity is no longer true globally. However, Xr,<sup>∞</sup> can be written as the union of compact sets spaces on which semiconcavity holds with an increasing κ.

![](_page_32_Figure_3.jpeg)

Figure 1: The green area denotes the metric space Xr,R. Geodesic curves are either straight lines like γ (1) or γ (3) or they touch the (light red) circle of radius r around the center (−r, −r) like γ (2). Considering γ (1) and γ (2) we see that geodesics can branch.

From the above examples one can see that the LAC and the semiconcavity of <sup>1</sup> 2 d 2 (xob, ·) serve the same purpose, namely controlling the divergence of initially identical or inially close geodesics for larger values. Consider two geodesics γ (j) : [0, 1] → X, j = 1, 2, that coincide for s ∈ [0, s∗] with s<sup>∗</sup> > 0 and differ for s > s∗. Splitting the two geodesics at the common point x = γ (1)(s∗) = γ (1)(s∗), we obtain three geodesics starting at x<sup>∗</sup> and ending in y<sup>0</sup> = γ (1)(0) = γ (1)(0), y<sup>1</sup> = γ (1)(1), and y<sup>2</sup> = γ (2)(1), respectively. Since γ (1) and γ (2) are geodesics, we easily see

$$
\sphericalangle(\widehat{xy_0}, \widehat{xy_1}) = \pi \quad \text{and} \quad \sphericalangle(\widehat{xy_0}, \widehat{xy_2}) = \pi.
$$

Thus, the LAC implies that ∢(xy<sup>1</sup> , xy<sup>2</sup> ) = 0, which means that the splitting of γ (1) and γ (2) has to be tangential. See Figure 5.4 in Example 5.12, where also the role of κ > 0 in the κ-concavity of <sup>1</sup> 2 d 2 (xob, ·) can be seen. In this example, κ is proportional to 1/r, which is the curvature of the arc contained in the geodesic γ (2). The exact statement is given in Proposition 5.7.

While the above simple examples provide some intuition, it is important to note that the 2-Wasserstein space (P(X), W2) is globally 1-concave, see Thm. 7.3.2+Prop. 9.3.12 in [AGS05]. The results needed for the theory developed here (see Theorem 6.6) for the spaces (M(X),HK) and (P(X), SHK) are provided in [LaM19, Prop. 4.1] for LAC and in [LaM19, Sec. 4.2] for semiconcavity. The latter results are local in the sense that one needs to restrict to suitable subsets with lower and upper density bounds.

### 5.5 Estimates for the MM scheme

In this subsection we state a few of the results from [Sav11, MuS23] quite explicitly, especially to emphasize the dependence on the semiconvexity parameter λ of φ and the semiconcavity parameter κ of <sup>1</sup> 2 d 2 . This concerns our Lemma 5.13, Proposition 5.14, Corollary 5.16, and Lemma 5.17.

We first state some very basic estimates that hold true for the discrete solutions of the MM scheme. Using geodesic interpolation, λ-convexity of φ and κ-concavity of the squared distance, one then obtains sharper estimates that allows us to control the distance between different approximants.

These estimates will be used later prove the existence of curves that satisfy EVIλ, but also to bound the distance of the EVI<sup>λ</sup> satisfying curve from the approximating curves occurring by geodesically interpolating the points of the minimizing scheme. Before we proceed we are going to define the metric slope |∂φ| of φ at a point x ∈ X via

$$
|\partial \phi|(x) = \begin{cases} +\infty & \text{for } x \notin \text{dom}(\phi), \\ 0 & \text{if } x \in \text{dom}(\phi) \text{ is isolated,} \\ \limsup_{y \to x} \frac{\max\{0, \phi(x) - \phi(y)\}}{d(x,y)} & \text{otherwise.} \end{cases}
$$
(5.12)

Lemma 5.13 (Euler equation for discrete solutions) *For a given* x τ <sup>n</sup>−<sup>1</sup> ∈ X *and* τ > 0, *assume that* x τ <sup>n</sup> ∈ X *is a solution to the minimizing movement scheme* (5.10)*. Then, for every observer* xob ∈ X *and all geodesics* x τ nx τ <sup>n</sup>−<sup>1</sup> ∈ Geod(x τ n , x<sup>τ</sup> n−1 ) *and* x τ <sup>n</sup>xob ∈ Geod(x τ n , xob), *we have*

$$
\frac{1}{\tau} \langle \widehat{x_n^{\tau} x_{n-1}^{\tau}}, \widehat{x_n^{\tau} x_{\text{ob}}} \rangle_{\text{up}} + \frac{\lambda}{2} \mathsf{d}_{\mathfrak{X}}^2(x_n^{\tau}, x_{\text{ob}}) + \phi(x_n^{\tau}) \le \phi(x_{\text{ob}}),\tag{5.13}
$$

(a) 
$$
|\partial \phi(x_n^{\tau})| \le \frac{d_{\mathfrak{X}}(x_{n-1}^{\tau}, x_n^{\tau})}{\tau},
$$
 (b) 
$$
(1 + \lambda \tau) \frac{d_{\mathfrak{X}}(x_{n-1}^{\tau}, x_n^{\tau})}{\tau} \le |\partial \phi|(x_{n-1}^{\tau}),
$$
 (5.14)  
(c) 
$$
(1 + \lambda \tau) |\partial \phi|(x_n^{\tau}) \le |\partial \phi|(x_{n-1}^{\tau}).
$$

We now provide an estimate of how much a geodesic interpolation of a minimizing scheme deviates from being an EVIλ<sup>∗</sup> solution for the modified λ <sup>∗</sup> = 2 min{0, λ}−2 ≤ −2, where λ is such that φ is λ-convex. We note and highlight that λ-convexity of φ with λ > 0 will not be helpful here. The following technical estimate will be used later to prove that interpolating curves x τ converge to some curve x when the time steps τ converges to 0. One important feature of the following result is that the error terms ∆<sup>τ</sup> n are independent of the observer point xob.

Proposition 5.14 (Discrete error estimates) *Let* λ ≤ 0*,* τ > 0 *with* 1+λτ > 0*, and* x τ <sup>0</sup> ∈ A<sup>κ</sup><sup>0</sup> *be fixed that the discrete solution* {x τ n }n∈**<sup>N</sup>** *of* (5.10) *satisfies the following: For* T > <sup>0</sup>, *let* <sup>κ</sup> <sup>∈</sup> **<sup>N</sup>** *be such that* <sup>x</sup> τ <sup>n</sup> <sup>∈</sup> <sup>A</sup>κ, *for all* <sup>n</sup> <sup>∈</sup> **<sup>N</sup>**<sup>0</sup> *with* n < <sup>T</sup> <sup>τ</sup> + 1*. Then, all geodesic interpolants* x τ : [0, ∞) → X*, given by*

$$
\boldsymbol{x}^{\tau}(t) = \widehat{x_n^{\tau} x_{n+1}^{\tau}} \left( (t - n\tau)/\tau \right) \text{ for } t \in [n\tau, (n+1)\tau] \quad \text{with } \widehat{x_n^{\tau} x_{n+1}^{\tau}} \in \text{Geod}(x_n^{\tau}, x_{n+1}^{\tau}), \tag{5.15}
$$

*satisfies, for all* xob ∈ AGeod κ *and almost all* t ∈ [0, T]*, the estimate*

$$
\frac{\mathrm{d}}{\mathrm{d}t} \frac{1}{2} \mathsf{d}_{\mathcal{X}}^2(\boldsymbol{x}^\tau(t), x_{\text{ob}}) + \frac{\lambda^*}{2} \mathsf{d}_{\mathcal{X}}^2(\boldsymbol{x}^\tau(t), x_{\text{ob}}) \le \phi(x_{\text{ob}}) - \phi(\boldsymbol{x}^\tau(t)) + \Delta^{\tau, \kappa}(t),\tag{5.16}
$$

*with* λ <sup>∗</sup> = 2 min{0, λ} − 2 *and* ∆<sup>τ</sup> (t) = ∆<sup>τ</sup> n *for* t ∈ [nτ,(n+1)τ )*, where*

$$
\Delta_n^{\tau,\kappa} = \begin{cases}\n(1-2\lambda)\mathsf{d}_{\mathfrak{X}}^2(x_0^{\tau}, x_1^{\tau}) + (1 + (1+\lambda\tau)^{-1})|\partial\phi|^2(x_0^{\tau}) & \text{for } n = 0, \\
(1-2\lambda+\kappa/\tau)\mathsf{d}^2(x_n^{\tau}, x_{n+1}^{\tau}) + \frac{1}{\tau^2}\Delta^2\left(\widehat{x_n^{\tau}x_{n-1}^{\tau}}, \widehat{x_n^{\tau}x_{n+1}^{\tau}}\right) & \text{for } n \in \mathbb{N}.\n\end{cases}
$$
\n(5.17)

The next result exploits the strength of the EVI formulation with an arbitrary observer. If we have two approximate solutions x<sup>i</sup> to the EVI, where the error term ∆i(t) does not depend on the observer, then we obtain a control on the distances between x<sup>1</sup> and x2.

Lemma 5.15 (Distance between approximate EVI solutions) *Let* ∆<sup>i</sup> *,* i = 1, 2*, be nonnegative real functions in* L 1 ([0, T])*. Let also* x<sup>i</sup> ∈ ACloc([0, ∞)), i = 1, 2, *be two locally absolutely continuous functions satisfying EVI*<sup>λ</sup> *for some* λ <sup>∗</sup> < 0 *in the form*

$$
\frac{\mathrm{d}}{\mathrm{d}t} \frac{1}{2} \mathsf{d}_{\mathcal{X}}^2(\boldsymbol{x}_i(t), x_{\text{ob}}) + \frac{\lambda^*}{2} \mathsf{d}_{\mathcal{X}}^2(\boldsymbol{x}_i(t), x_{\text{ob}}) \leq \phi(x_{\text{ob}}) - \phi(\boldsymbol{x}_i(t)) + \Delta_i(t), \tag{5.18}
$$

*for a.a.* t ∈ [0, T] *and every* xob ∈ x3−i([0, T]) *for* i = 1, 2*. Then we have the estimate*

$$
\sup_{t\in[0,T]} e^{\lambda^* t} d_{\mathfrak{X}}(\boldsymbol{x}_1(t),\boldsymbol{x}_2(t)) \leq d_{\mathfrak{X}}(\boldsymbol{x}_1(0),\boldsymbol{x}_2(0)) + ||2e^{2\lambda^* t} (\Delta_1 + \Delta_2)||_{L^1[0,T]}^{1/2}
$$
(5.19)

Proof. For s, t <sup>∈</sup> [0, T] we define the function <sup>q</sup>(s, t) := <sup>1</sup> 2 dX(x1(s), x2(t)). Applying [AGS05, Lem. 4.3.4] we obtain the differential inequality

$$
\frac{d}{dt} \frac{1}{2} d_{\mathcal{X}}^{2}(\boldsymbol{x}_{1}(t), \boldsymbol{x}_{2}(t)) \leq \limsup_{h \downarrow 0} \frac{q(t, t) - q(t - h, t)}{h} + \limsup_{h \downarrow 0} \frac{q(t, t) - q(t, t + h)}{h}
$$
\n
$$
= \frac{d}{ds} \frac{1}{2} d_{\mathcal{X}}^{2}(\boldsymbol{x}_{1}(s), \boldsymbol{x}_{2}(t))|_{s=t} + \frac{d}{dr} \frac{1}{2} d_{\mathcal{X}}^{2}(\boldsymbol{x}_{1}(t), \boldsymbol{x}_{2}(r))|_{r=t} \leq -\lambda^{*} d_{\mathcal{X}}^{2}(\boldsymbol{x}_{1}(t), \boldsymbol{x}_{2}(t)) + \Delta_{1}(t) + \Delta_{2}(t), \tag{5.20}
$$

for almost every t > 0, where we used (5.18) for x<sup>i</sup> with observer xob = x3−<sup>i</sup> . Multiplying the inequality by 2e<sup>2</sup><sup>λ</sup> ∗t and integrating in time yields the desired estimate.

The following result specifies this estimate by looking at the solutions obtained as geodesic interpolants from MM schemes with two different time steps τ > 0 and σ > 0.

Corollary 5.16 (Comparison of MM solutions) *Let* τ, σ, *two time steps and let* x τ *and* x σ *be two piecewise geodesic interpolants defined in* (5.15) *for initial conditions* x τ 0 , x<sup>σ</sup> <sup>0</sup> ∈ A<sup>κ</sup><sup>0</sup> *, respectively. Let* φ *be* λ*-convex with* λ ≤ 0 *and set* λ <sup>∗</sup> = 2 min{0, λ} − 2*. Then we have*

$$
\sup_{t\in[0,T]} e^{\lambda^* t} \mathsf{d}_{\mathfrak{X}}(\mathbf{x}^\tau(t), \mathbf{x}^\sigma(t)) \leq \mathsf{d}_{\mathfrak{X}}(x_0^\tau, x_0^\sigma) + ||2e^{2\lambda^* t} (\Delta^\tau + \Delta^\sigma)||_{L^1[0,T]}^{1/2},\tag{5.21}
$$

*where* ∆<sup>τ</sup> *and* ∆<sup>σ</sup> *are defined via* (5.17) *in Proposition 5.14.*

Thus, it remains to control the error functions ∆<sup>τ</sup> . The main problem is to control the terms <sup>1</sup> τ **∆** 2 x τ nx τ n−1 , x τ nx τ <sup>n</sup>+1 , which control the change of the "directions and length" of the connecting geodesic interpolants. For this we use the improved incremental energy estimate (5.13) which allows us to invoke a telescope sum by inserting xob = x τ <sup>n</sup>+1.

Lemma 5.17 (Controlling the incremental errors) *Let* λ ≤ 0*,* τ ∈ (0, 1) *with* τλ > −1/2*,* x τ <sup>0</sup> ∈ A<sup>κ</sup><sup>0</sup> , *and let* x τ n *be defined iteratively by the MM scheme. For* T > 0 *and* κ ≥ κ<sup>0</sup> *assume* x τ <sup>n</sup> ∈ A<sup>κ</sup> *as long as* n < T/τ + 1*. Then, the error function* ∆<sup>τ</sup> *defined in* (5.17) *in Proposition 5.14 satisfies the estimate*

$$
\|e^{2\lambda^*t}\Delta^{\tau}\|_{L^1([0,T])} \le \tau\left(4+\tau\kappa\right)|\partial\phi|^2(x_0^{\tau}),\tag{5.22}
$$

*where we recall that* λ <sup>∗</sup> = 2 min{0, λ} − 2.

### 5.6 Proof of the main abstract result in Theorem 5.9

Having prepared the above the preliminary estimates for the solutions x τ n of the minimizing movement scheme, we are now ready to give the proof of the abstract existence result. The important point in the proof is that the value κ of the κ-concavity of d 2 is occurring only in a few places that are well controlled. In particular, it is needed only on the time-discrete level (see e.g. (5.6) and (5.17)), but it disappears in the EVI formulation.

#### Proof of Theorem 5.9.

Let λ such that φ is geodesically λ-convex, then φ is also geodesically min{λ, 0}-convex. As before we set λ <sup>∗</sup> = 2 min{λ, 0} − 2.

Step 1. Limit passage on approximate solutions x <sup>τ</sup><sup>k</sup> . We now exploit the assumptions (A1) to (A3) of Theorem 5.9. For a given initial point <sup>x</sup><sup>0</sup> ∈ ∪κ∈**<sup>N</sup>**A<sup>κ</sup> there exist <sup>κ</sup><sup>0</sup> <sup>∈</sup> **<sup>N</sup>** such that T(x0, κ) > 0 for all κ > κ0. Fixing a κ<sup>∗</sup> > κ<sup>0</sup> we define time steps τ<sup>k</sup> = T/k for <sup>k</sup> <sup>∈</sup> **<sup>N</sup>**, where <sup>T</sup> <sup>=</sup> <sup>T</sup>(x0, κ∗) from assumption (A2). Hence, we have <sup>x</sup> τk <sup>n</sup> ∈ A<sup>κ</sup><sup>∗</sup> as long as n < T/τ<sup>k</sup> + 1. By construction of the geodesic interpolants in (5.15) the function x <sup>τ</sup><sup>k</sup> : [0, T] → X satisfies x <sup>τ</sup><sup>k</sup> (t) ∈ AGeod κ∗ for all t ∈ [0, T].

Thus, we are able to apply Corollary 5.16 and Lemma 5.17 and obtain (for τk, τ<sup>k</sup> ′ ≤ 1)

$$
\sup_{t\in[0,T]} \mathsf{d}_{\mathfrak{X}}(\mathbf{x}^{\tau_k}(t),\mathbf{x}^{\tau_{k'}}(t)) \leq e^{-\mathfrak{X}^T 2} \left( (\tau_k + \tau_{k'})(4 + \kappa_*) \right)^{1/2} |\partial \phi|(x_0). \tag{5.23}
$$

Therefore the curves x <sup>τ</sup><sup>k</sup> : [0, T] → X converge uniformly in the compact and hence complete sublevel y ∈ X  φ(y) ≤ φ(x0) to a continuous limiting curve x : [0, T] → X with x(0) = x0.

Step 2. x is the unique EVIλ<sup>∗</sup> solution. We now return to the approximate EVI formulation (5.16) for the interpolants x τk .

For a general observer, by assumption A3, for xob ∈ dom(φ) we can choose a sequence (ym)m∈**<sup>N</sup>** with

$$
y_m \in A_{\kappa_m} \subset \mathfrak{X}, \quad y_m \stackrel{m \to \infty}{\longrightarrow} x_{\text{ob}}, \quad \phi(y_m) \stackrel{m \to \infty}{\longrightarrow} \phi(x_{\text{ob}}).
$$

Without loss of generality we may assume κ<sup>∗</sup> ≤ κm. Choosing xob = y<sup>m</sup> ∈ A<sup>κ</sup><sup>m</sup> ⊂ AGeod κm in (5.16) for x <sup>τ</sup><sup>k</sup> and integrating over the interval (s, t) we find

$$
\frac{1}{2} \mathsf{d}_{\mathcal{X}}^2(\mathbf{x}^{\tau_k}(t), y_m) - \frac{1}{2} \mathsf{d}_{\mathcal{X}}^2(\mathbf{x}^{\tau_k}(s), y_m) + \int_s^t \left( \phi(\mathbf{x}^{\tau_k}(r)) + \frac{\lambda^*}{2} \mathsf{d}_{\mathcal{X}}^2(\mathbf{x}^{\tau_k}(r), y_m) \right) dr
$$
\n
$$
\leq (t-s)\phi(y_m) + (t-s)e^{-2\lambda^*t}\tau_k(4+\tau_k\kappa_m) |\partial\phi|^2(x_0).
$$
\n(5.24)

Keeping m fixed, taking k → ∞, and using the lower semicontinuity of φ, we obtain

$$
\frac{1}{2}d_{\mathfrak{X}}^2(\boldsymbol{x}(t),y_m)-\frac{1}{2}d_{\mathfrak{X}}^2(\boldsymbol{x}(s),y_m)+\int_s^t\left(\phi(\boldsymbol{x}(r))+\frac{\lambda^*}{2}d_{\mathfrak{X}}^2(\boldsymbol{x}(r),y_m)\right)\mathrm{d}r\leq (t-s)\phi(y_m),
$$

Note that κ<sup>m</sup> has disappeared because of τ<sup>k</sup> → 0 for k → ∞. Now m → ∞ yields

$$
\frac{1}{2}d_{\mathfrak{X}}^2(\boldsymbol{x}(t),x_{\text{ob}})-\frac{1}{2}d_{\mathfrak{X}}^2(\boldsymbol{x}(s),x_{\text{ob}})+\int_s^t\left(\phi(\boldsymbol{x}(r))+\frac{\lambda^*}{2}d_{\mathfrak{X}}^2(\boldsymbol{x}(r),x_{\text{ob}})\right)\mathrm{d}r\leq (t-s)\phi(x_{\text{ob}}),
$$

where we have convergence on the left-hand side and use the lsc of φ on the right-hand side. As the inequality trivially holds for xob ∈ X\dom(φ), we have shown that x : [0, T] is an EVIλ<sup>∗</sup> solution. The uniqueness follows by applying Corollary 5.16 with ∆<sup>τ</sup> = ∆<sup>σ</sup> = 0.

Step 3. Extension to t ∈ [0, T∞(x0)). In the previous step the solution x : [0, T(x0, κ∗)] → X was well-defined and unique. However, κ<sup>∗</sup> > κ0(x0) was arbitrary. Hence, we can extend the solution uniquely to any interval [0, T(x0, κ)] with κ > κ0. Taking the limit κ → ∞ we obtain a unique solution on [0, T∞(x0)) ⊂ ∪κ>κ<sup>0</sup> [0, T(x0, κ)].

Step 4. Complete EVI flow on dom(φ). We now further assume T∞(x0) = ∞ for all x<sup>0</sup> ∈ ∪κAκ. We now consider an arbitrary x<sup>0</sup> ∈ dom(φ). Since ∪A<sup>κ</sup> = dom(φ), there exists a sequence (x m 0 )m∈**<sup>N</sup>** with x m <sup>0</sup> ∈ A<sup>κ</sup><sup>m</sup> ⊂ ∪κ∈**<sup>N</sup>**A<sup>κ</sup> and x <sup>m</sup> <sup>m</sup>→∞ −→ <sup>x</sup>. Define <sup>x</sup> <sup>m</sup> : [0, ∞) → X to be the unique EVIλ<sup>∗</sup> solution starting in x m 0 . By (5.9) in Proposition 5.8(C), we have

$$
e^{\lambda^*t}\mathbf{d}_{\mathfrak{X}}(\boldsymbol{x}^m(t),\boldsymbol{x}^{m'}(t)) \leq \mathbf{d}_{\mathfrak{X}}(\boldsymbol{x}_0^m,\boldsymbol{x}_0^{m'}) \text{ for all } t \geq 0.
$$

Thus, for all T > 0 the sequence x <sup>m</sup> is Cauchy in the space C([0, T]; X). Therefore, it converges locally uniform to a limit x : [0, ∞) → X, which satisfies the initial condition x(0) = x<sup>0</sup> = lim x m 0 . Since each curve x <sup>m</sup> satisfies the integrated form (5.8) of EVIλ<sup>∗</sup> , the lower semicontinuity of φ guarantees that the limit curve x is again an EVI solution.

Step 5. Correcting λ <sup>∗</sup> back to λ. Above we have constructed EVIλ<sup>∗</sup> solutions, but our functional φ is geodesically λ-convex and λ > λ<sup>∗</sup> . To recover the correct λ, we can apply [MuS20, Cor. 3.12] because we know that x is an EVIλ<sup>∗</sup> solution for (X, d, φ) and that φ is λ-convex with λ ≥ λ ∗ . Hence, x is also EVI<sup>λ</sup> solution.

# 6 Semiconcavity and EVI flows for (M(X), E, HK) and (P(X), E, SHK)

We now combine the theory developed in the previous two sections, namely the existence result for EVI flows provided in Theorem 5.9 with the semiconcavity results established in [LaM19, Sec. 4].

#### 6.1 Semiconcavity of <sup>1</sup> 2 HK<sup>2</sup> and <sup>1</sup> 2 SHK<sup>2</sup>

In order to apply Theorem 5.9 in the case of HK, SHK, we need to provide some semiconcavity results. More specifically, we need to prove that point (A1) is satisfied for a sequence of sets Aκ. Before we proceed, we will define the following two collections of sets. For δ ∈ (0, 1) we define the set

$$
\mathcal{M}_{\delta}(X) = \left\{ \mu \in \mathcal{M}(X) : \mu \ll \mathcal{L}^d, \ \delta \le \frac{d\mu}{d\mathcal{L}^d}(x) \le \frac{1}{\delta}, \text{ for } \mathcal{L}^d \text{-a.e. } x \in X \right\}.
$$
 (6.1)

For positive numbers d1, d2, we also define

$$
\widetilde{\mathcal{M}}_{d_1,d_2}(X) = \left\{ \mu \in \mathcal{M}(X) : \forall x \in X : d_2 \le \frac{\mu\left(B\left(x, \mathbf{d}_1\right)\right)}{\mathcal{L}^d(B\left(x, \mathbf{d}_1\right))} \le \frac{1}{\mathbf{d}_2} \right\}.
$$
\n(6.2)

It is straightforward to see that for all <sup>d</sup><sup>1</sup> <sup>&</sup>gt; <sup>0</sup> it holds <sup>M</sup>δ(X) <sup>⊂</sup> <sup>M</sup><sup>e</sup> <sup>d</sup>1,δ(X). Furthermore all elements in Mδ(X) have total mass bounded by <sup>1</sup> δ L d (X).

In [LaM19, Thm. 4.8], it was stated and proved that for a set <sup>X</sup> <sup>⊂</sup> **<sup>R</sup>** d that is compact, convex and with nonempty interior, there exists <sup>κ</sup>(δ) <sup>∈</sup> **<sup>R</sup>**, such that (M(X),HK) is <sup>κ</sup>concave on Mδ(X). We clarify at this point, that in practice Theorem 4.8 was stated for more general metric spaces and for reference measures ν that are doubling. However for simplification we are going to recall any theorems or lemmas we need from [LaM19] directly adapted to to our setting, avoiding all the extra generality related to doubling measures and abstract metric spaces. We remind the reader, that for a compact, convex set <sup>X</sup> <sup>⊂</sup> **<sup>R</sup>** <sup>d</sup> with nonempty interior, the Lebesgue measure is doubling and the Euclidean distance is 2-concave. Although [LaM19, Thm. 4.8] was stated in this weaker form, the given proof provides a stronger result, namely the following:

Theorem 6.1 (K-concavity for (M(X),HK)) *Let* <sup>X</sup> <sup>⊂</sup> **<sup>R</sup>** d *be a compact, convex set with nonempty interior. Then, there exists* <sup>κ</sup>(δ) <sup>∈</sup> **<sup>R</sup>**, *such that* HK *is* <sup>κ</sup>*-concave on* Mδ(X), *with respect to observers in* MGeod δ (X).

At the moment of writing [LaM19], we were not aware that this version will be useful, however now this property along with LAC condition, is exactly the assumption (A1) in our Theorem 5.9. LAC condition was proven in [LaM19, Theorem 4.1]. Now, we will recall some lemmas from there and provide a short proof of Theorem 6.1.

Lemma 6.2 ([LaM19, Lem. 4.9]) *Let* <sup>X</sup> <sup>⊂</sup> **<sup>R</sup>** d *be a compact, convex set with nonempty interior. There exists* 0 < Cmin ≤ Cmax *such that for every* µ0, µ<sup>1</sup> ∈ Me d1,d<sup>2</sup> (X) *and any optimal plan* <sup>H</sup><sup>01</sup> *for* LETd(· ; <sup>µ</sup>0, µ1) *we have*

$$
C_{\min} \le \sigma_i(x_i) \le C_{\max}, \quad \eta_i \text{-}a.e. \tag{6.3}
$$

*where* η<sup>i</sup> = π i #H<sup>01</sup> = σiµ<sup>i</sup> *for* i = 0, 1*. Furthermore, any transportation happens in distances strictly less than some* <sup>π</sup> 2 , *i.e. there exists* D < π 2 *that depends only on* d1, d2, *such that* <sup>d</sup>X(x0, x1) <sup>≤</sup> <sup>D</sup> *for* <sup>H</sup><sup>01</sup> *almost every* (x0, x1).

Lemma 6.3 ([LaM19, Lem. 4.10]) *Let* <sup>X</sup> <sup>⊂</sup> **<sup>R</sup>** d *be a compact, convex set with nonempty interior and* <sup>M</sup>δ(X) *be as in* (6.1)*. Then, for each* δ > <sup>0</sup> *there exist* <sup>d</sup><sup>1</sup> <sup>∈</sup> (0, π 2 ) *and* d<sup>2</sup> > 0 *such that any constant-speed geodesic* µ<sup>01</sup> *connecting* µ<sup>0</sup> *to* µ<sup>1</sup> *with* µ0, µ<sup>1</sup> ∈ Mδ(X) *satisfies* µ01(t) ∈ Me d1,d<sup>2</sup> (X) *for all* t ∈ [0, 1].

From Lemma 6.3 we obtain

$$
\mathcal{M}_{\delta}(X) \subset \mathcal{M}_{\delta}^{\text{Geod}}(X) \subset \widetilde{\mathcal{M}}_{d_1, d_2}(X).
$$

For the proof of [LaM19, Thm. 4.8] also the following result is used.

Lemma 6.4 ([LaM19, Lem. 4.11]) *Let* <sup>X</sup> <sup>⊂</sup> **<sup>R</sup>** d *be a compact, convex set with nonempty interior and* Me d1,d<sup>2</sup> (X) *be as in* (6.2)*. Then, there exist* Rmin, Rmax > 0 *that depend on* <sup>d</sup>1, <sup>d</sup>2, *such that for* <sup>µ</sup>0, µ<sup>1</sup> *with* <sup>µ</sup>01(t) <sup>∈</sup> <sup>M</sup><sup>e</sup> d1,d<sup>2</sup> (X) *and* µ<sup>2</sup> ∈ Me d1,d<sup>2</sup> (X) *we can find measures* λ0, λ1, λ2, λ<sup>t</sup> ∈ P2(C[Rmin, Rmax]) *with*

$$
\mathfrak{P}\lambda_i = \mu_i, \quad \mathfrak{P}\lambda_t = \mu_{01}(t), \quad \mathcal{W}_{d_{\mathfrak{C}}}(\lambda_i, \lambda_t) = \mathsf{HK}(\mu_i, \mu_{01}(t)) \quad \text{ for } i = 0, 1, 2.
$$

The observer's location does not necessarily have to be within Mδ, as shown in both Lemma 6.4 and the actual proof of [LaM19, Thm.,4.8]. It is sufficient for the observer to be situated in Me d1,d<sup>2</sup> (X), which includes MGeod δ (X). Consequently, the same proof can be applied without any changes.

Proof of Theorem 6.1. By Lemma 6.3 there exists 0 < d<sup>1</sup> < π 2 and 0 < d<sup>2</sup> such that every geodesic µ<sup>01</sup> connecting µ0, µ<sup>1</sup> ∈ Mδ(X) satisfies µ01(t) ∈ Me d1,d<sup>2</sup> (X) for all t ∈ [0, 1]. We also have µ<sup>2</sup> ∈ Me d1,d<sup>2</sup> (X) ⊃ M<sup>δ</sup> Geod(X).

We would like to utilize the equivalent definitions of K-concavity provided in [LaM19, Cor. 2.24(iii)] where a function <sup>f</sup> : [0, 1] <sup>→</sup> **<sup>R</sup>** is <sup>K</sup>-concave if for every <sup>t</sup>1, t<sup>2</sup> <sup>∈</sup> [0, 1] with t<sup>1</sup> < t<sup>2</sup> the mapping ˜f [t1,t2] i (t) = f<sup>i</sup> (t1+t(t2−t1)) satisfies

$$
\tilde{f}_i^{[t_1, t_2]}(t) + Kt(1-t)(t_2 - t_1)^2 \ge (1-t)\tilde{f}_i^{[t_1, t_2]}(0) + t\tilde{f}_i^{[t_1, t_2]}(1) \text{ for all } t \in [0, 1].\tag{6.4}
$$

In that direction, we take µ˜<sup>0</sup> = µ01(t1), µ˜<sup>1</sup> = µ01(t2) for t1, t<sup>2</sup> ∈ [0, 1], and µ˜01(t) = <sup>µ</sup>01(t(t2−t1) + <sup>t</sup>1). By Lemma 6.4, there exists <sup>R</sup>min, Rmax that depend on <sup>d</sup>1, <sup>d</sup>2, and therefore on δ, such that for every µ˜0, µ˜1, µ˜<sup>2</sup> ∈ Me d1,d<sup>2</sup> (X) and 0 < t < 1 we can find measures λ0, λ1, λ2, λ<sup>t</sup> ∈ P2(C[Rmin, Rmax]) with

$$
\mathfrak{P}\lambda_i = \tilde{\mu}_i, \quad \mathfrak{P}\lambda_t = \tilde{\mu}_{01}(t), \text{ and } W_{d_c}(\lambda_i, \lambda_t) = \mathsf{HK}(\tilde{\mu}_i, \tilde{\mu}_{01}(t)), \quad i = 0, 1, 2,
$$
\n
$$
(6.5)
$$

see Theorem 3.2. Using the geodesic property of µ˜<sup>01</sup> yields

$$
W_{d_{\mathfrak{C}}}(\lambda_0, \lambda_t) + W_{d_{\mathfrak{C}}}(\lambda_1, \lambda_t) = \mathsf{HK}(\mu_0, \tilde{\mu}_{01}(t)) + \mathsf{HK}(\mu_1, \tilde{\mu}_{01}(t))
$$
  
=  $\mathsf{HK}(\tilde{\mu}_0, \tilde{\mu}_1) \leq W_{d_{\mathfrak{C}}}(\lambda_0, \lambda_1).$ 

Hence, it is straightforward to see that there exists a geodesic λ<sup>01</sup> connecting λ0, λ1, such that λ01(t) = λ<sup>t</sup> . Furthermore, by [Lis06, Thm. 6] there is a plan Λ0→<sup>1</sup> on the geodesics such that Λts := (e<sup>t</sup> , es)♯Λ0→<sup>1</sup> is an optimal plan between λ(t) and λ(s). Now, by using a gluing lemma, we can find a plan Λ0→<sup>1</sup> 2t in P((C[0, 1]; C) × C), such that Λ<sup>01</sup> = (e0, e1)<sup>♯</sup> π 0→1 <sup>♯</sup> Λ0→<sup>1</sup> 2t , and (et(π 0→1 ) <sup>×</sup> <sup>I</sup>)♯Λ0→<sup>1</sup> 2t is an optimal plan for W<sup>d</sup><sup>C</sup> (λ2, λ01(t)). Finally by applying the last part of Lemma 6.2, we get the existence of a D < π 2 such that |x2−x<sup>t</sup> | < D for (et(π 0→1 ) <sup>×</sup> <sup>I</sup>)♯Λ0→<sup>1</sup> 2t almost every (z2, zt), similarly |x0−x1| < D for Λ<sup>01</sup> almost every [z0, z1]. Therefore, for Λ0→<sup>1</sup> <sup>2</sup><sup>t</sup> almost every (z2, z(·, z0, z1)), where z(·, z0, z1) is a geodesic connecting z0, z1, we have x0, x1, x2, x¯(t, z0, z1) ∈ B (x¯(t, z0, z1), d). By [LaM19, Prop. 2.27] we get a K′ such that

$$
\mathsf{d}_{\mathfrak{C}}^2(z_2, \mathbf{z}(t, z_0, z_1)) + K't(1-t)\mathsf{d}_{\mathfrak{C}}^2(z_0, z_1) \ge (1-t)\mathsf{d}_{\mathfrak{C}}^2(z_2, z_0) + t\mathsf{d}_{\mathfrak{C}}^2(z_2, z_1),\qquad(6.6)
$$

for Λ0→<sup>1</sup> 2t almost every (z2, <sup>z</sup>(·, z0, z1)). By integrating with respect to <sup>Λ</sup>0→<sup>1</sup> 2t , we find

$$
W_{d_{\mathfrak{C}}}^2(\lambda_2, \lambda_{01}(t)) + K't(1-t)W_{d_{\mathfrak{C}}}^2(\lambda_0, \lambda_1) \ge (1-t)W_{d_{\mathfrak{C}}}^2(\lambda_2, \lambda_0) + t W_{d_{\mathfrak{C}}}^2(\lambda_2, \lambda_1). \tag{6.7}
$$

Using (6.5) we find the desired semiconcavity, and Theorem 6.1 is proved.

# 6.2 Geodesic semiconvexity of functionals on HK and SHK

In [LMS23] the question of geodesic λ-convexity of functionals E with reference measure L <sup>d</sup> on a d-dimensional domain are discussed in detail. It is shown that E defined as in (1.1) in terms of a lsc and convex density functions E with E(0) = 0 is λ-convex on (M(X),HK) if and only if the auxiliary function

$$
N_{E,\lambda} : (0,\infty)^2 \to \mathbb{R} \cup \{\infty\}; \ (\rho,\gamma) \mapsto \left(\frac{\rho}{\gamma}\right)^d E\left(\frac{\gamma^{2+d}}{\rho^d}\right) - \frac{\lambda}{2}\gamma^2
$$

satisfies the following two conditions:

$$
N_{E,\lambda}: (0,\infty)^2 \to \mathbb{R} \cup \{\infty\} \text{ is convex and}
$$
  
$$
\rho \mapsto (d-1) N_{E,\lambda}(\rho,\gamma) \text{ is non-increasing.}
$$
 (6.8)

It is shown that the density functions E of the form

$$
E(c) = \alpha_0 c + \alpha_1 c^{p_1} + \dots + \alpha_m c^{p_m}
$$

lead to geodesically <sup>2</sup>α0-convex <sup>E</sup> if <sup>α</sup><sup>0</sup> <sup>∈</sup> **<sup>R</sup>** and <sup>α</sup><sup>i</sup> <sup>≥</sup> <sup>0</sup> and <sup>p</sup><sup>i</sup> <sup>&</sup>gt; <sup>1</sup> for <sup>i</sup> = 1, . . . , m. Moreover in dimensions d ∈ {1, 2} the density function E(c) = −βc<sup>q</sup> with β ≥ 0 and <sup>q</sup> <sup>∈</sup> [d/(d+2), <sup>1</sup>/2] lead to geodesically convex functionals <sup>E</sup>.

So far there doesn't seem to be a theory for semiconvexity on P(X), SHK) which can be used to provide examples. In Appendix A we establish the following nontrivial class of examples.

Proposition 6.5 *Consider dimension* <sup>d</sup> ∈ {1, <sup>2</sup>} *and a bounded convex domain* <sup>X</sup> <sup>⊂</sup> **<sup>R</sup>** d *that is the closure of an open set. Then, the functional* E<sup>q</sup> *defined via*

$$
\mathsf{E}_q(\mu) = -\int_X \rho^q \, \mathrm{d}x \quad \text{for } \mu = \rho \, \mathrm{d}x + \mu^\perp
$$

*is geodesically convex on* (P(X), SHK) *if* <sup>q</sup> <sup>∈</sup> [d/(d+2), <sup>1</sup>/2]*.*

With this result we are sure that the following main existence result for EVI flows on (P(X), E, SHK) provides at least the solutions to a small, but nontrivial family of nonlinear partial differential equations. Following the discussion in Section 2, the partial differential equations associated with such functionals. For the case q = 1/2 we explicitly obtain the nonlinear PDE

$$
\dot{\rho} = \frac{1}{2}\Delta\sqrt{\rho} + 2\left(\sqrt{\rho} - \rho \int_X \sqrt{\rho} \,dx\right) \text{ in } X, \quad \nabla\rho \cdot \nu = 0 \text{ on } \partial X
$$

which defines a contraction semiflow (EVI flow) in the space P(X), SHK).

### 6.3 The Main Result

In this section we collect the results from the previous sections and provide the proof of our main result, which we repeat here for convenience.

Theorem 6.6 *Let* <sup>X</sup> <sup>⊂</sup> **<sup>R</sup>** d *be a compact, convex set with nonempty interior. Furthermore, let* E *be a functional defined as in* (1.1) *that satisfies Assumption A.*

*Then, for all* µ<sup>0</sup> = ρ0L <sup>d</sup> *with* <sup>0</sup> < ρ<sup>0</sup> ≤ ρ0(x) ≤ ρ<sup>0</sup> < ∞ *a.e. in* X*, the geodesically interpolated solutions of the MM schemes corresponding to the gradient system* (M(X), E,HK) *(*(P(X), E, SHK)*), as in* (1.2)HK *(* (1.2)SHK*), converge to a complete solution* <sup>µ</sup> : [0, <sup>∞</sup>) <sup>→</sup> <sup>P</sup>(X) *of EVI*λ*. Moreover, for all* <sup>µ</sup><sup>0</sup> <sup>∈</sup> dom(E) HK ⊂ M(X) *(* µ<sup>0</sup> ∈ dom(E) SHK ⊂ P(X)*) there exists a unique EVI solution, emanating from* µ0*.*

Proof. We start from µ<sup>0</sup> = ρ0L <sup>d</sup> with <sup>0</sup> < ρ<sup>0</sup> ≤ ρ0(x) ≤ ρ<sup>0</sup> < ∞. By Propositions 4.4 (for SHK) and 4.6 (for HK) we know that for every T there exists a τ<sup>0</sup> > 0, such that for all τ ≤ τ<sup>0</sup> and n with nτ < T, the solutions µ<sup>n</sup> of the MM scheme satisfy µ<sup>n</sup> = ρnL <sup>d</sup> where 0 < δ ≤ ρn(x) ≤ 1/δ < ∞. But this means that µ<sup>n</sup> ∈ Mδ(X) for all n with nτ < T. Therefore, by taking A<sup>κ</sup> = Mδ(X) and using Theorem 6.1 we see that all assumptions of Theorem 5.9 are satisfied. We note that LAC was proved in [LaM19, Theorem 4.1]. Therefore, the convergence of the MM scheme for density restricted initial data µ<sup>0</sup> follows.

For general initial data µ<sup>0</sup> in the closure dom(E) SHK <sup>⊂</sup> <sup>P</sup>(X) or dom(E) HK ⊂ M(X) of the domain of the functional E we can choose approximations µ m <sup>0</sup> = ρ m <sup>0</sup> L d satisfying ρ m <sup>0</sup> ∈ [1/m, m] a.e. in X. The associated EVI solutions µ<sup>m</sup> are complete and converge to the desired, complete EVI solution emanating from µ<sup>0</sup> by applying (5.19).

# A Transfer of λ-convexity between HK and SHK

We rely on the interpretation of (M(X),HK) as a cone of over (P(X), SHK) that was developed in [LaM19]. But first we consider a general geodesic space (X, d) and the associated cone (C, D), which take the places of (P(X), SHK) and M(X),HK), respectively.

We present a general result that demonstrates, under appropriate conditions, the geodesic convexity of a p-homogeneous functional F on (C, D). Specifically, we show that when F is restricted to (X, d), it remains geodesically convex.

Proposition A.1 (Transfer for negative, homogeneous functionals) *Assume that* <sup>F</sup> : <sup>C</sup> <sup>→</sup> [−∞, 0] *is geodesically convex on* (C, D)*, and that it is* <sup>p</sup>*-homogeneous for some* p ≥ 1/2*, i.e.*

$$
\mathsf{F}([x,r]) = r^p \mathsf{F}([x,1]) \text{ for all } [x,r] \in \mathcal{C}.
$$

*Moreover, assume* <sup>d</sup>(x, y) < π *for all* x, y <sup>∈</sup> <sup>X</sup>*.*

*Then,* <sup>E</sup>(x) := <sup>F</sup>([x, 1]) <sup>∈</sup> [−∞, 0] *is geodesically convex on* (X, <sup>d</sup>)*.*

Proof. We use the fact that all geodesics x : [0, 1] → X connecting x and x<sup>1</sup> are given by the geodesics z : [0, 1] 7→ [x(t), r(t)] ∈ C by a simple reparametrization, see [LaM19, Thm. 2.7], namely, setting δ = d(x, x1) ∈ ]0, π[ we have

$$
x(t) = \overline{x}(\beta_{\delta}(t)) \text{ with } \beta_{\delta}(t) := \frac{\sin(t\delta)}{\sin(t\delta) + \sin((1-t)\delta)},
$$

where βδ(0) = 0 and βδ(1) = 1. Moreover, r(t) = 1 − t(1−t)D([x, 1], [x1, 1])<sup>2</sup> with <sup>D</sup>([x, 1], [x1, 1]) = (2(1<sup>−</sup> cos <sup>δ</sup>))<sup>1</sup>/<sup>2</sup> can be rewritten via

$$
r(\beta_{\delta}(t)) = r_{\delta}(t) := \frac{\sin(\delta)}{\sin(t\delta) + \sin((1-t)\delta)} \in [1/2, 1],
$$

where r(0) = r(1) = 1. With this we obtain

$$
\mathsf{E}(x(t)) = \mathsf{F}([x(t), 1])
$$
 (definition of  $\mathsf{E}$ )  
\n
$$
= \mathsf{F}([\overline{x}(\beta_{\delta}(t)), 1])
$$
 (reparametrization)  
\n
$$
= \frac{1}{r_{\delta}(t)^p} \mathsf{F}([\overline{x}(\beta_{\delta}(t)), r_{\delta}(t)])
$$
 (p-homogeneity of  $\mathsf{F}$ )  
\n
$$
\leq \frac{1}{r_{\delta}(t)^p} ((1 - \beta_{\delta}(t)) \mathsf{F}([\overline{x}(0), r(0)]) + \beta_{\delta}(t) \mathsf{F}([\overline{x}(1), r(1)]))
$$
 (geodesic cvx of  $\mathsf{F}$  on  $\mathcal{C}$ )  
\n
$$
= \frac{1 - \beta_{\delta}(t)}{r_{\delta}(t)^p} \mathsf{E}(x) + \frac{\beta_{\delta}(t)}{r_{\delta}(t)^p} \mathsf{E}(x_1)
$$
 (definition of  $\mathsf{E}$ )

Because of <sup>E</sup>(x<sup>j</sup> ) <sup>≤</sup> <sup>0</sup> it suffices to show the two estimates

$$
\frac{1-\beta_{\delta}(t)}{r_{\delta}(t)^p} \ge 1 - t \quad \text{and} \quad \frac{\beta_{\delta}(t)}{r_{\delta}(t)^p} \ge t \qquad \text{for all } t \in [0, 1] \text{ and } \delta \in [0, \pi]. \tag{A.1}
$$

For the second estimate and the case p ≥ 1 we use the explicit form and obtain

$$
\frac{\beta_{\delta}(t)}{r_{\delta}(t)^{p}} = \frac{1}{r_{\delta}(t)^{p-1}} \frac{\sin(t\delta)}{\sin\delta} \ge t \quad \text{for all } t \in [0, 1],
$$

where we used rβ(t) ≤ 1 and that t 7→ sin(tδ) is concave on [0, 1] because of δ ∈ ]0, π[. Thus, the result certainly holds for p ≥ 1.

However, it also holds for p ∈ [1/2, 1] by the following arguments. Define the function

$$
Q_p(t,\delta) = \frac{\sin(t\delta)}{t(\sin\delta)^p} \left(\sin(t\delta) + \sin((1-t)\delta)\right)^{p-1}.
$$

It suffices to show Qp(t, δ) ≥ 1 for t ∈ ]0, 1[ and δ ∈ ]0, π[. Clearly, we have Qp(1, δ) = 1 and we find

$$
\partial_t Q_p(1,\delta) = -1 + p \frac{\delta \cos \delta}{\sin \delta} + (1-p) \frac{\delta}{\sin \delta}.
$$

For p ∈ [1/2, 1] one can show that ∂tQp(t, δ) ≤ 0 which implies Qp(t, δ) ≥ Qp(1, δ) = 1 which is the desired second estimate in (A.1).

To see why p ≥ 1/2 is necessary, we use limδ→π<sup>−</sup> (sin δ ∂tQp(1, δ)) = π(1−2p). Thus, for p < 1/2 we have ∂tQp(t, δ) > 0 for δ ≈ π, which implies Qp(t, δ) < 1.

The first estimate in (A.1) follows similarly, namely by changing t to 1−t. This proves the result.

We now consider (M(X),HK) as the cone over (P(X), SHK) for some convex and compact <sup>X</sup> <sup>⊂</sup> **<sup>R</sup>** d . We first observe that [LaM19, Thm. 3.4] guarantees SHK(ν0, ν1) <sup>≤</sup> π/<sup>2</sup> that the condition d(x, y) < π is automatically satisfied.

From [LMS23] we know that the functionals

$$
\mathsf{E}_q(\mu) = \int_X \varrho(x)^q \, \mathrm{d}x \quad \text{ for } \mu = \varrho \, \mathrm{d}x
$$

are geodesically 0-convex on (M(X),HK) whenever q > 1. Moreover, in the sense of cones we have µ = r <sup>2</sup>̺L <sup>d</sup> giving p-homogeneity with p = 2q, namely

$$
\mathsf{E}_q(\mu) = \mathsf{E}_q(r^2 \varrho \mathcal{L}^d) = \mathsf{F}_q([\varrho \mathcal{L}^d, r]) = r^{2q} \mathsf{F}_q([\varrho \mathcal{L}^d, 1]) = r^{2q} \mathsf{E}_q(\varrho \mathcal{L}^d).
$$

However, the above result is not applicable because of <sup>F</sup>q(µ) <sup>≥</sup> <sup>0</sup>.

Note also that the special case q = 1 leads to the mass functional EM(µ) = E1(µ) = R X µ(dx) = µ(X) which is geodesically 2-convex for HK (as well as geodesically 2-concave). However, its spherical restriction is obviously constant, hence it is geodesically 0-convex and 0-concave. This means that we have a drop in the convexity, namely

$$
0 = \Lambda_{\mathbf{SK}} \subsetneqq \Lambda_{\mathbf{IK}} = 2.
$$

However, the above result can be applied in the case of functionals of the form

$$
\mathsf{E}_q(\mu) = -\int_X \varrho(x)^q dx \quad \text{for } q \in (0,1) \text{ and } \mu = \varrho \mathcal{L}^d + \mu^\perp,
$$
 (A.2)

where µ <sup>⊥</sup> is singular with respect to L d . This leads to the

Proof of Proposition 6.5. For <sup>ν</sup>0, ν<sup>1</sup> <sup>∈</sup> <sup>P</sup>(X) we first observe HK<sup>2</sup> (ν0, ν1) ≤ ν0(X) + ν1(X) which implies SHK(ν0, ν1) = 2 arcsin( <sup>1</sup> 2 HK(ν0, ν1)) <sup>≤</sup> 2 arcsin( <sup>1</sup> 2 √ 2) = π/2 < π.

It is shown in [LMS23] that E<sup>q</sup> is geodesically 0-convex under the following conditions:

$$
q \in [1/3, 1/2]
$$
 and  $d = 1$  or  $q = 1/2$  and  $d = 2$ .

Moreover, we obviously have <sup>E</sup>q(µ) <sup>≤</sup> <sup>0</sup> and <sup>E</sup>q(<sup>r</sup> <sup>2</sup>̺L d ) = Fq([̺L d , r]) = r <sup>2</sup>qFq([̺L d , 1]) = r <sup>2</sup>qEq(̺ L d ). Using q ≥ 1/3 we have p-homogeneity with p = 2q ≥ 2/3. Hence, all assumptions of Proposition A.1 are satisfied, and the geodesic convexity of E<sup>q</sup> restricted to (P(X), SHK) follows.

Acknowledgments. The authors are grateful to Giuseppe Savaré for fruitful and stimulating discussions and for sharing the preprint [Sav11]. V. Laschos was supported by DFG, through the project EXC-2046/1 within Germany´s Excellence Strategy – The Berlin Mathematics Research Center MATH+ (project ID: 390685689). A. Mielke was partially supported by DFG through the project Mie 459/9-1 within the Priority Program SPP 2256 *Variational Methods for Predicting Complex Phenomena in Engineering Structures and Materials* (project ID: 441470105).

# References

- [AFP00] L. Ambrosio, N. Fusco, and D. Pallara: Functions of bounded variation and free discontinuity problems. Clarendon Press, Oxford, 2000.
- [AGS05] L. Ambrosio, N. Gigli, and G. Savaré, Gradient flows in metric spaces and in the space of probability measures, Lectures in Mathematics ETH Zürich, Birkhäuser Verlag, Basel, 2005.
- [CP<sup>∗</sup>15] L. Chizat, G. Peyré, B. Schmitzer, and F.-X. Vialard: An interpolating distance between optimal transport and Fisher–Rao metrics. Found. Comput. Math. 18:1 (2015) 1–44.
- [CPSV18] L. Chizat, G. Peyré, B. Schmitzer and F.-X. Vialard. Unbalanced optimal transport: Dynamic and Kantorovich formulations. Journal of Functional Analysis, 274(11):3090–3123, 2018.
- [DaS14] S. Daneri and G. Savaré. Lecture notes on gradient flows and optimal transport. Chapter 6 in "Optimal Transportation. Theory and Applications, Y. Ollivier, H. Pajot, C. Villani (eds), Cambridge Univ. Press (2014)", pp. 100–144 (and arXiv:1009.3737v1).
- [DiC20] S. Di Marino and L. Chizat: A tumor growth model of Hele-Shaw type as a gradient flow. ESAIM Control Optim. Calc. Var. 26:103 (2020) 1–38.
- [Fle21] F. C. Fleißner. A minimizing movement approach to a class of scalar reaction–diffusion equations. ESAIM: Control, Optimisation and Calculus of Variations, 27:18, 2021.

- [GaM17] T. O. Gallouët and L. Monsaingeon: A JKO splitting scheme for Kantovorich-Fisher-Rao gradient flows. SIAM J. Math. Analysis 49:2 (2017) 1100–1130.
- [GLM19] T. Gallouët, M. Laborde and L. Monsaingeon. An unbalanced optimal transport splitting scheme for general advection-reaction-diffusion problems. ESAIM: Control, Optimisation and Calculus of Variations, 25:8, 2019.
- [HLP34] G. H. Hardy, J. E. Littlewood, and G. Pólya: Inequalities. Cambridge University Press, 1934.
- [JKO98] R. Jordan, D. Kinderlehrer, and F. Otto: The variational formulation of the Fokker-Planck equation. SIAM J. Math. Analysis 29:1 (1998) 1–17.
- [KMV16a] S. Kondratyev, L. Monsaingeon and D. Vorotnikov. A fitness-driven cross-diffusion system from population dynamics as a gradient flow. Journal of Differential Equations, 261(5):2784–2808, 2016.
- [KMV16b] S. Kondratyev, L. Monsaingeon and D. Vorotnikov. A new optimal transport distance on the space of finite Radon measures. Advances in Differential Equations, 21(11/12):1117 – 1164, 2016.
- [KV19] S. Kondratyev and D. Vorotnikov. Spherical Hellinger–Kantorovich gradient flows. SIAM Journal on Mathematical Analysis, 51(3):2053–2084, 2019.
- [LaM19] V. Laschos and A. Mielke: Geometric properties of cones with applications on the Hellinger– Kantorovich space, and a new distance on the space of probability measures. J. Funct. Analysis 276:11 (2019) 3529–3576.
- [LiM13] M. Liero and A. Mielke: Gradient structures and geodesic convexity for reaction-diffusion systems. Phil. Trans. Royal Soc. A 371:2005 (2013) 20120346, 28.
- [Lis06] S. Lisini. Characterization of absolutely continuous curves in Wasserstein spaces. Calc. Var. Partial Differ. Eqns. 28:1 (2006) 85–120.
- [LMS16] M. Liero, A. Mielke, and G. Savaré: Optimal transport in competition with reaction the Hellinger–Kantorovich distance and geodesic curves. SIAM J. Math. Analysis 48:4 (2016) 2869– 2911.
- [LMS18] : Optimal entropy-transport problems and a new Hellinger–Kantorovich distance between positive measures. Invent. math. 211 (2018) 969–1117.
- [LMS23] : Fine properties of geodesics and geodesic λ-convexity for the Hellinger–Kantorovich distance. Arch. Rat. Mech. Analysis, to appear; arXiv2208.14299v3 Sept. 26, (2023).
- [Mie23] A. Mielke: An Introduction to the Analysis of Gradient Systems. Lecture notes from HU Berlin, Winter 2022/2023, arXiv:2306.05025 (2023).
- [MuS20] M. Muratori and G. Savaré: Gradient flows and evolution variational inequalities in metric spaces. I: structural properties. J. Funct. Analysis 278:4 (2020) 108347.
- [MuS23] M. Muratori and G. Savaré: Gradient flows and evolution variational inequalities in metric spaces. II: variational convergence and III: generation results. In preparation, 2023.
- [Ott96] F. Otto, Double degenerate diffusion equations as steepest descent, Preprint no. 480, SFB 256, University of Bonn, 1996.
- [Ott01] : The geometry of dissipative evolution equations: the porous medium equation. Comm. Partial Diff. Eqns. 26 (2001) 101–174.
- [PQV14] B. Perthame, F. Quirós, and J. L. Vázquez: The Hele-Shaw asymptotics for mechanical models of tumor growth. Arch. Rational Mech. Anal. 212:1 (2014) 93–127.
- [Sav07] G. Savaré, Gradient flows and diffusion semigroups in metric spaces under lower curvature bounds, C. R. Math. Acad. Sci. Paris 345:3 (2007) 151–154.
- [Sav11] G. Savaré, Gradient flows and diffusion semigroups in metric spaces under lower curvature bounds. Extended version of [Sav07]. Private Communication, 2011.